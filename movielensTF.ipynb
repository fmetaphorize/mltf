{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "O1AltDoVqwBN",
    "outputId": "5cb20e05-a016-423f-b112-d227d9f62306",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_YTLOZqwBQ"
   },
   "source": [
    "## 导入数据以及数据概览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "collapsed": true,
    "id": "yzZ79qD-qwBR",
    "outputId": "54ab1378-28e1-40e7-9840-f2616705826a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-aad2e6ad-d863-486f-a0c8-8c3719eeb68a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aad2e6ad-d863-486f-a0c8-8c3719eeb68a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-aad2e6ad-d863-486f-a0c8-8c3719eeb68a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-aad2e6ad-d863-486f-a0c8-8c3719eeb68a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入users.dat文件\n",
    "#为这个dat文件中每一列取相应的名字\n",
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "collapsed": true,
    "id": "21sWpVxUqwBR",
    "outputId": "a73fa9de-e20c-4bb6-f755-6bbe60b82f64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ae28e47f-c277-42d7-8035-014d7eae2a65\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae28e47f-c277-42d7-8035-014d7eae2a65')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ae28e47f-c277-42d7-8035-014d7eae2a65 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ae28e47f-c277-42d7-8035-014d7eae2a65');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入movies.dat文件\n",
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vq9PGc0aqwBS",
    "outputId": "daf543ff-aa71-4fc0-e3a5-1fdff3bdec42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7dfe9f01-93f5-455e-9cfd-33601417f0bb\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dfe9f01-93f5-455e-9cfd-33601417f0bb')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7dfe9f01-93f5-455e-9cfd-33601417f0bb button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7dfe9f01-93f5-455e-9cfd-33601417f0bb');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##导入ratings.dat文件\n",
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiYS34NHqwBS"
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eqXWqi0TqwBT"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #处理表示用户性别的变量Gender\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "    #处理表示用户年龄的变量Age\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "\n",
    "    #电影类型变量Genres转数字字典\n",
    "    genres_set = set()\n",
    "    #原文件中若一部电影有多个风格，使用的|作为并列符号，所以分离时split('|')\n",
    "    for val in movies['Genres'].str.split('|'): \n",
    "        genres_set.update(val)\n",
    "    #空白部分用‘< PAD >’对应的数字进行填充，这样方便将长度统一，之后有利于在神经网络中进行处理。\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型变量Genres转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')#利用正则表达式去掉title中的年份信息\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    #空白部分用‘< PAD >’对应的数字进行填充，这样方便将长度统一，之后有利于在神经网络中进行处理。\n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取rating.dat文件\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "    #合并三个dat表，方便后续进行处理\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    #将数据分成X(已有的所有特征)和y(目标变量rating)两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    features = features_pd.values      #这是所有的特征值\n",
    "    targets_values = targets_pd.values #这是目标变量rating值\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z1dCTbReqwBT"
   },
   "outputs": [],
   "source": [
    "#运行上方的对数据进行预处理的函数load_data()\n",
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3jD9pToqwBT"
   },
   "source": [
    "### 查看预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IqoHbAsgqwBT",
    "outputId": "4b12ed2e-3e0a-45a6-e3b0-98c4ef2717d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-36424834-14fd-425b-a23f-3b6f75d9c7b5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36424834-14fd-425b-a23f-3b6f75d9c7b5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-36424834-14fd-425b-a23f-3b6f75d9c7b5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-36424834-14fd-425b-a23f-3b6f75d9c7b5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Ld8KhvoIqwBT",
    "outputId": "983dba70-f362-4bf6-be90-72d538708da9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7355de46-6fe6-449f-8de9-e622b3c4b8b6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[2936, 4367, 4037, 4037, 4037, 4037, 4037, 403...</td>\n",
       "      <td>[11, 16, 3, 15, 15, 15, 15, 15, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4796, 4037, 4037, 4037, 4037, 4037, 4037, 403...</td>\n",
       "      <td>[8, 16, 5, 15, 15, 15, 15, 15, 15, 15, 15, 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[5063, 1553, 2868, 4037, 4037, 4037, 4037, 403...</td>\n",
       "      <td>[3, 7, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[140, 3577, 5102, 4037, 4037, 4037, 4037, 4037...</td>\n",
       "      <td>[3, 18, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[4117, 32, 1412, 618, 3980, 3217, 4037, 4037, ...</td>\n",
       "      <td>[3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7355de46-6fe6-449f-8de9-e622b3c4b8b6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7355de46-6fe6-449f-8de9-e622b3c4b8b6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7355de46-6fe6-449f-8de9-e622b3c4b8b6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   MovieID  ...                                             Genres\n",
       "0        1  ...  [11, 16, 3, 15, 15, 15, 15, 15, 15, 15, 15, 15...\n",
       "1        2  ...  [8, 16, 5, 15, 15, 15, 15, 15, 15, 15, 15, 15,...\n",
       "2        3  ...  [3, 7, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,...\n",
       "3        4  ...  [3, 18, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15...\n",
       "4        5  ...  [3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfFj-oScqwBU",
    "outputId": "27f7bc62-93db-482c-8d53-d8b094203c70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([2936, 4367, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037, 4037]),\n",
       "       list([11, 16, 3, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "XHXUEyxwqwBU"
   },
   "outputs": [],
   "source": [
    "# 从本地读取数据\n",
    "title_count, title_set, genres2int, features, targets_values,ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWH0PhR-qwBV"
   },
   "source": [
    "## 一些有关存储与读取的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Jpnie7fZqwBV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    将参数保存至文件params.p中\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    从params.p文件中加载数据\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pu4F-oHqwBV"
   },
   "source": [
    "## 设定和嵌入相关的固定数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oy_8TJB1qwBV"
   },
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "#对电影类型嵌入向量做加和操作的标志\n",
    "combiner = \"sum\"\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bspeqVIqwBV"
   },
   "source": [
    "### 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ptsuRIwjqwBV"
   },
   "outputs": [],
   "source": [
    "# 训练的轮次数目 ：epoch的数目\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "#drop的大小，防止过拟合\n",
    "dropout_keep = 0.5\n",
    "# 学习率learning rate\n",
    "learning_rate = 0.0001\n",
    "# 设定每20个batch就显示一次输出\n",
    "show_every_n_batches = 20\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoFFKu1mqwBV"
   },
   "source": [
    "### 输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWfqlVDgqwBV"
   },
   "source": [
    "定义输入的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "77lJMCHDqwBV"
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tapfrz5-qwBW"
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRzbaePsqwBW"
   },
   "source": [
    "#### User的embedding：对应将有关用户的原始特征输入嵌入层的那一步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TuzR7PsyqwBW"
   },
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        #uid_embed_matrix是一个被随即均匀随机数填充的维度是[uid_max, embed_dim]的张量\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        #在uid_embed_matrix的张量列表中执行并行查找，返回和uid_embed_matrix相同类型的tensor，这便是经过嵌入层后得到的userID向量\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "        #对用户性别变量Gender执行同样操作\n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        #对用户年龄变量Age执行同样操作\n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        #对用户职业变量job执行同样操作\n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kXotT3-qwBW"
   },
   "source": [
    "#### 将和用户相关的embedding送入接下来的两层全连接层(FC)，训练得到用户特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e60k2GxBqwBW"
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接层，将通过get_user_embedding()得到的四个变量的嵌入矩阵分别送入第一层全连接层进行训练\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)#使用relu作为激活函数\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接，将上方从全连接层输出的四个变量的向量进行拼接(concat)之后送入第二层全连接层\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "        #reshape：变化tensor的维度，使得到的用户特征向量与电影特征向量一致且正确\n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RlbzJk3qwBW"
   },
   "source": [
    "#### MovieID的embedding(嵌入矩阵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Wd8zxQ3yqwBW"
   },
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szp7wgOLqwBW"
   },
   "source": [
    "#### 电影类型Genres的embedding(嵌入矩阵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UbEePxqlqwBW"
   },
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1),\n",
    "                                                    name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories,\n",
    "                                                              name = \"movie_categories_embed_layer\")\n",
    "        #对电影类型的多个嵌入向量做加和（因为一部电影的风格类型有多种，本实验采用加和的方式进行处理）\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "网络的第一层是词嵌入层，由每一个单词的嵌入向量组成的嵌入矩阵。\n",
    "下一层使用多个不同尺寸（窗口大小）的卷积核在嵌入矩阵上做卷积，窗口大小指的是每次卷积覆盖几个单词。\n",
    "这里跟对图像做卷积不太一样，图像的卷积通常用2x2、3x3、5x5之类的尺寸，而文本卷积要覆盖整个单词的嵌入向量，所以尺寸是（单词数，向量维度），\n",
    "比如每次滑动3个，4个或者5个单词。第三层网络是max pooling得到一个长向量，\n",
    "最后使用dropout做正则化，最终得到了电影Title的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b40xek-eqwBX"
   },
   "source": [
    "#### 电影名称变量Title的文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mDi3a0j6qwBX"
   },
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        #先得到嵌入矩阵embedding\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        #reshape改变其维度，将其展平\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    #文本卷积覆盖整个单词的嵌入向量，所以尺寸是（单词数，向量维度）\n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积以及最大池化maxpooling\n",
    "    pool_layer_lst = [] # 用来存放每次池化后的结果\n",
    "    #window_sizes = {2, 3, 4, 5} 文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词  \n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            #卷积核权重W\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            #卷积核偏置b\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            #卷积conv\n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            #做一次relu\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            #maxpooling最大池化\n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    # Dropout\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")#将pool_layer_lst中存放的四个结果拼接起来\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeGYhp5vqwBX"
   },
   "source": [
    "#### 将上述的得到的和电影相关的三个变量的嵌入矩阵一起送入全连接层，得到电影特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B8iH-WHQqwBX"
   },
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接层  仍然使用relu作为激活函数\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接层\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "        #重塑tenso的维度\n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khUDM6FuqwBX"
   },
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5n-b50rqwBX",
    "outputId": "1572ce0d-5774-4d12-8ee1-8bc3b2fb58b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-dc08624e5fcc>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-559a1ee9ce9e>:6: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-21-bb012f2abe28>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #用get_user_embedding获取User的4个嵌入向量，然后用get_user_feature_layer得到用户特征\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #用get_movie_id_embed_layer获取电影ID的嵌入向量，然后用get_movie_categories_layers获取电影类型的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量，然后得到电影特征\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,movie_categories_embed_layer,dropout_layer)\n",
    "    #计算出评分rating，注意后期做推荐时，传入的inference有两个不同的值，因此这里会有两个不同的方案\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1) #这里得到预测评分\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )# MSE损失，将计算值回归到评分\n",
    "        loss = tf.reduce_mean(cost)\n",
    "        \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)#采用Adam优化算法，lr：学习率learning rate\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tk8hKPIAqwBX",
    "outputId": "3caec471-25fa-440e-a650-de19137cc284"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8czlwz4qwBY"
   },
   "source": [
    "## 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9_uNX1-xqwBY"
   },
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_KB3fglqwBY"
   },
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91CQCVTUqwBY",
    "outputId": "a65c3965-6cb4-402b-c17b-e1ce8e4da8f9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /content/runs/1640521250\n",
      "\n",
      "2021-12-26T12:20:59.208584: Epoch   0 Batch    0/3125   train_loss = 21.708\n",
      "2021-12-26T12:20:59.997538: Epoch   0 Batch   20/3125   train_loss = 5.651\n",
      "2021-12-26T12:21:00.779154: Epoch   0 Batch   40/3125   train_loss = 3.053\n",
      "2021-12-26T12:21:01.560477: Epoch   0 Batch   60/3125   train_loss = 2.084\n",
      "2021-12-26T12:21:02.292780: Epoch   0 Batch   80/3125   train_loss = 2.298\n",
      "2021-12-26T12:21:03.057890: Epoch   0 Batch  100/3125   train_loss = 1.957\n",
      "2021-12-26T12:21:03.813986: Epoch   0 Batch  120/3125   train_loss = 2.107\n",
      "2021-12-26T12:21:04.574307: Epoch   0 Batch  140/3125   train_loss = 1.814\n",
      "2021-12-26T12:21:05.338989: Epoch   0 Batch  160/3125   train_loss = 1.581\n",
      "2021-12-26T12:21:06.074545: Epoch   0 Batch  180/3125   train_loss = 1.682\n",
      "2021-12-26T12:21:06.841178: Epoch   0 Batch  200/3125   train_loss = 1.643\n",
      "2021-12-26T12:21:07.593032: Epoch   0 Batch  220/3125   train_loss = 1.650\n",
      "2021-12-26T12:21:08.375567: Epoch   0 Batch  240/3125   train_loss = 1.538\n",
      "2021-12-26T12:21:09.132735: Epoch   0 Batch  260/3125   train_loss = 1.511\n",
      "2021-12-26T12:21:09.902016: Epoch   0 Batch  280/3125   train_loss = 1.510\n",
      "2021-12-26T12:21:10.674066: Epoch   0 Batch  300/3125   train_loss = 1.716\n",
      "2021-12-26T12:21:11.426735: Epoch   0 Batch  320/3125   train_loss = 1.608\n",
      "2021-12-26T12:21:12.169843: Epoch   0 Batch  340/3125   train_loss = 1.398\n",
      "2021-12-26T12:21:12.932136: Epoch   0 Batch  360/3125   train_loss = 1.344\n",
      "2021-12-26T12:21:13.697235: Epoch   0 Batch  380/3125   train_loss = 1.349\n",
      "2021-12-26T12:21:14.461064: Epoch   0 Batch  400/3125   train_loss = 1.292\n",
      "2021-12-26T12:21:15.233743: Epoch   0 Batch  420/3125   train_loss = 1.261\n",
      "2021-12-26T12:21:15.983102: Epoch   0 Batch  440/3125   train_loss = 1.413\n",
      "2021-12-26T12:21:16.737005: Epoch   0 Batch  460/3125   train_loss = 1.384\n",
      "2021-12-26T12:21:17.531373: Epoch   0 Batch  480/3125   train_loss = 1.375\n",
      "2021-12-26T12:21:18.342696: Epoch   0 Batch  500/3125   train_loss = 1.004\n",
      "2021-12-26T12:21:19.138163: Epoch   0 Batch  520/3125   train_loss = 1.415\n",
      "2021-12-26T12:21:20.093669: Epoch   0 Batch  540/3125   train_loss = 1.276\n",
      "2021-12-26T12:21:21.536548: Epoch   0 Batch  560/3125   train_loss = 1.421\n",
      "2021-12-26T12:21:23.043334: Epoch   0 Batch  580/3125   train_loss = 1.354\n",
      "2021-12-26T12:21:24.630517: Epoch   0 Batch  600/3125   train_loss = 1.479\n",
      "2021-12-26T12:21:26.245370: Epoch   0 Batch  620/3125   train_loss = 1.542\n",
      "2021-12-26T12:21:27.684905: Epoch   0 Batch  640/3125   train_loss = 1.456\n",
      "2021-12-26T12:21:29.356470: Epoch   0 Batch  660/3125   train_loss = 1.462\n",
      "2021-12-26T12:21:30.803899: Epoch   0 Batch  680/3125   train_loss = 1.173\n",
      "2021-12-26T12:21:32.367761: Epoch   0 Batch  700/3125   train_loss = 1.411\n",
      "2021-12-26T12:21:33.981753: Epoch   0 Batch  720/3125   train_loss = 1.150\n",
      "2021-12-26T12:21:35.479137: Epoch   0 Batch  740/3125   train_loss = 1.431\n",
      "2021-12-26T12:21:36.945534: Epoch   0 Batch  760/3125   train_loss = 1.259\n",
      "2021-12-26T12:21:38.278365: Epoch   0 Batch  780/3125   train_loss = 1.352\n",
      "2021-12-26T12:21:39.578284: Epoch   0 Batch  800/3125   train_loss = 1.330\n",
      "2021-12-26T12:21:40.339552: Epoch   0 Batch  820/3125   train_loss = 1.249\n",
      "2021-12-26T12:21:41.117503: Epoch   0 Batch  840/3125   train_loss = 1.207\n",
      "2021-12-26T12:21:41.867318: Epoch   0 Batch  860/3125   train_loss = 1.203\n",
      "2021-12-26T12:21:42.620816: Epoch   0 Batch  880/3125   train_loss = 1.209\n",
      "2021-12-26T12:21:43.392883: Epoch   0 Batch  900/3125   train_loss = 1.252\n",
      "2021-12-26T12:21:44.175298: Epoch   0 Batch  920/3125   train_loss = 1.338\n",
      "2021-12-26T12:21:44.947641: Epoch   0 Batch  940/3125   train_loss = 1.425\n",
      "2021-12-26T12:21:45.783149: Epoch   0 Batch  960/3125   train_loss = 1.264\n",
      "2021-12-26T12:21:46.750168: Epoch   0 Batch  980/3125   train_loss = 1.410\n",
      "2021-12-26T12:21:47.500922: Epoch   0 Batch 1000/3125   train_loss = 1.270\n",
      "2021-12-26T12:21:48.289513: Epoch   0 Batch 1020/3125   train_loss = 1.323\n",
      "2021-12-26T12:21:49.141406: Epoch   0 Batch 1040/3125   train_loss = 1.283\n",
      "2021-12-26T12:21:49.892366: Epoch   0 Batch 1060/3125   train_loss = 1.433\n",
      "2021-12-26T12:21:50.676014: Epoch   0 Batch 1080/3125   train_loss = 1.149\n",
      "2021-12-26T12:21:51.461698: Epoch   0 Batch 1100/3125   train_loss = 1.382\n",
      "2021-12-26T12:21:52.217268: Epoch   0 Batch 1120/3125   train_loss = 1.288\n",
      "2021-12-26T12:21:53.330121: Epoch   0 Batch 1140/3125   train_loss = 1.274\n",
      "2021-12-26T12:21:54.980066: Epoch   0 Batch 1160/3125   train_loss = 1.306\n",
      "2021-12-26T12:21:56.363353: Epoch   0 Batch 1180/3125   train_loss = 1.246\n",
      "2021-12-26T12:21:57.924232: Epoch   0 Batch 1200/3125   train_loss = 1.228\n",
      "2021-12-26T12:21:59.350199: Epoch   0 Batch 1220/3125   train_loss = 1.163\n",
      "2021-12-26T12:22:00.706855: Epoch   0 Batch 1240/3125   train_loss = 1.136\n",
      "2021-12-26T12:22:02.073993: Epoch   0 Batch 1260/3125   train_loss = 1.214\n",
      "2021-12-26T12:22:03.527562: Epoch   0 Batch 1280/3125   train_loss = 1.239\n",
      "2021-12-26T12:22:05.040688: Epoch   0 Batch 1300/3125   train_loss = 1.229\n",
      "2021-12-26T12:22:06.396109: Epoch   0 Batch 1320/3125   train_loss = 1.304\n",
      "2021-12-26T12:22:07.855116: Epoch   0 Batch 1340/3125   train_loss = 1.046\n",
      "2021-12-26T12:22:09.150237: Epoch   0 Batch 1360/3125   train_loss = 1.153\n",
      "2021-12-26T12:22:10.470169: Epoch   0 Batch 1380/3125   train_loss = 1.135\n",
      "2021-12-26T12:22:11.580791: Epoch   0 Batch 1400/3125   train_loss = 1.292\n",
      "2021-12-26T12:22:12.822332: Epoch   0 Batch 1420/3125   train_loss = 1.278\n",
      "2021-12-26T12:22:14.242980: Epoch   0 Batch 1440/3125   train_loss = 1.200\n",
      "2021-12-26T12:22:16.006719: Epoch   0 Batch 1460/3125   train_loss = 1.241\n",
      "2021-12-26T12:22:17.462560: Epoch   0 Batch 1480/3125   train_loss = 1.260\n",
      "2021-12-26T12:22:18.877267: Epoch   0 Batch 1500/3125   train_loss = 1.383\n",
      "2021-12-26T12:22:20.068975: Epoch   0 Batch 1520/3125   train_loss = 1.244\n",
      "2021-12-26T12:22:21.279744: Epoch   0 Batch 1540/3125   train_loss = 1.352\n",
      "2021-12-26T12:22:22.060853: Epoch   0 Batch 1560/3125   train_loss = 1.167\n",
      "2021-12-26T12:22:22.813127: Epoch   0 Batch 1580/3125   train_loss = 1.329\n",
      "2021-12-26T12:22:23.607982: Epoch   0 Batch 1600/3125   train_loss = 1.277\n",
      "2021-12-26T12:22:24.372737: Epoch   0 Batch 1620/3125   train_loss = 1.160\n",
      "2021-12-26T12:22:25.164047: Epoch   0 Batch 1640/3125   train_loss = 1.351\n",
      "2021-12-26T12:22:25.917992: Epoch   0 Batch 1660/3125   train_loss = 1.319\n",
      "2021-12-26T12:22:26.683781: Epoch   0 Batch 1680/3125   train_loss = 1.228\n",
      "2021-12-26T12:22:27.453726: Epoch   0 Batch 1700/3125   train_loss = 1.104\n",
      "2021-12-26T12:22:28.193375: Epoch   0 Batch 1720/3125   train_loss = 1.176\n",
      "2021-12-26T12:22:28.955825: Epoch   0 Batch 1740/3125   train_loss = 1.229\n",
      "2021-12-26T12:22:30.537531: Epoch   0 Batch 1760/3125   train_loss = 1.269\n",
      "2021-12-26T12:22:31.266334: Epoch   0 Batch 1780/3125   train_loss = 1.134\n",
      "2021-12-26T12:22:32.011389: Epoch   0 Batch 1800/3125   train_loss = 1.214\n",
      "2021-12-26T12:22:32.760412: Epoch   0 Batch 1820/3125   train_loss = 1.242\n",
      "2021-12-26T12:22:33.535836: Epoch   0 Batch 1840/3125   train_loss = 1.331\n",
      "2021-12-26T12:22:34.289112: Epoch   0 Batch 1860/3125   train_loss = 1.270\n",
      "2021-12-26T12:22:35.031470: Epoch   0 Batch 1880/3125   train_loss = 1.304\n",
      "2021-12-26T12:22:35.797564: Epoch   0 Batch 1900/3125   train_loss = 1.071\n",
      "2021-12-26T12:22:36.548040: Epoch   0 Batch 1920/3125   train_loss = 1.159\n",
      "2021-12-26T12:22:37.343267: Epoch   0 Batch 1940/3125   train_loss = 1.111\n",
      "2021-12-26T12:22:38.083229: Epoch   0 Batch 1960/3125   train_loss = 1.176\n",
      "2021-12-26T12:22:38.840270: Epoch   0 Batch 1980/3125   train_loss = 1.126\n",
      "2021-12-26T12:22:39.597789: Epoch   0 Batch 2000/3125   train_loss = 1.457\n",
      "2021-12-26T12:22:40.388363: Epoch   0 Batch 2020/3125   train_loss = 1.296\n",
      "2021-12-26T12:22:41.118284: Epoch   0 Batch 2040/3125   train_loss = 1.124\n",
      "2021-12-26T12:22:41.872092: Epoch   0 Batch 2060/3125   train_loss = 1.022\n",
      "2021-12-26T12:22:42.619904: Epoch   0 Batch 2080/3125   train_loss = 1.254\n",
      "2021-12-26T12:22:43.369214: Epoch   0 Batch 2100/3125   train_loss = 1.170\n",
      "2021-12-26T12:22:44.100864: Epoch   0 Batch 2120/3125   train_loss = 1.039\n",
      "2021-12-26T12:22:44.836979: Epoch   0 Batch 2140/3125   train_loss = 1.146\n",
      "2021-12-26T12:22:45.622009: Epoch   0 Batch 2160/3125   train_loss = 1.116\n",
      "2021-12-26T12:22:46.384622: Epoch   0 Batch 2180/3125   train_loss = 1.149\n",
      "2021-12-26T12:22:47.157488: Epoch   0 Batch 2200/3125   train_loss = 1.107\n",
      "2021-12-26T12:22:47.897105: Epoch   0 Batch 2220/3125   train_loss = 1.125\n",
      "2021-12-26T12:22:48.641258: Epoch   0 Batch 2240/3125   train_loss = 0.998\n",
      "2021-12-26T12:22:49.400078: Epoch   0 Batch 2260/3125   train_loss = 1.155\n",
      "2021-12-26T12:22:50.188785: Epoch   0 Batch 2280/3125   train_loss = 1.248\n",
      "2021-12-26T12:22:50.973367: Epoch   0 Batch 2300/3125   train_loss = 1.181\n",
      "2021-12-26T12:22:51.722172: Epoch   0 Batch 2320/3125   train_loss = 1.329\n",
      "2021-12-26T12:22:52.497255: Epoch   0 Batch 2340/3125   train_loss = 1.145\n",
      "2021-12-26T12:22:53.261007: Epoch   0 Batch 2360/3125   train_loss = 1.179\n",
      "2021-12-26T12:22:54.029291: Epoch   0 Batch 2380/3125   train_loss = 1.091\n",
      "2021-12-26T12:22:54.789095: Epoch   0 Batch 2400/3125   train_loss = 1.226\n",
      "2021-12-26T12:22:55.552843: Epoch   0 Batch 2420/3125   train_loss = 1.156\n",
      "2021-12-26T12:22:56.319598: Epoch   0 Batch 2440/3125   train_loss = 1.247\n",
      "2021-12-26T12:22:57.070126: Epoch   0 Batch 2460/3125   train_loss = 1.147\n",
      "2021-12-26T12:22:57.851773: Epoch   0 Batch 2480/3125   train_loss = 1.252\n",
      "2021-12-26T12:22:58.576123: Epoch   0 Batch 2500/3125   train_loss = 1.204\n",
      "2021-12-26T12:22:59.365881: Epoch   0 Batch 2520/3125   train_loss = 1.139\n",
      "2021-12-26T12:23:00.114914: Epoch   0 Batch 2540/3125   train_loss = 1.037\n",
      "2021-12-26T12:23:00.889352: Epoch   0 Batch 2560/3125   train_loss = 0.942\n",
      "2021-12-26T12:23:01.596251: Epoch   0 Batch 2580/3125   train_loss = 1.179\n",
      "2021-12-26T12:23:02.379083: Epoch   0 Batch 2600/3125   train_loss = 1.173\n",
      "2021-12-26T12:23:03.141126: Epoch   0 Batch 2620/3125   train_loss = 1.107\n",
      "2021-12-26T12:23:03.874833: Epoch   0 Batch 2640/3125   train_loss = 1.111\n",
      "2021-12-26T12:23:04.640347: Epoch   0 Batch 2660/3125   train_loss = 1.224\n",
      "2021-12-26T12:23:05.399816: Epoch   0 Batch 2680/3125   train_loss = 1.045\n",
      "2021-12-26T12:23:06.162980: Epoch   0 Batch 2700/3125   train_loss = 1.245\n",
      "2021-12-26T12:23:06.902872: Epoch   0 Batch 2720/3125   train_loss = 1.159\n",
      "2021-12-26T12:23:07.689760: Epoch   0 Batch 2740/3125   train_loss = 1.189\n",
      "2021-12-26T12:23:08.459352: Epoch   0 Batch 2760/3125   train_loss = 1.223\n",
      "2021-12-26T12:23:09.205086: Epoch   0 Batch 2780/3125   train_loss = 1.067\n",
      "2021-12-26T12:23:09.938355: Epoch   0 Batch 2800/3125   train_loss = 1.415\n",
      "2021-12-26T12:23:10.682700: Epoch   0 Batch 2820/3125   train_loss = 1.414\n",
      "2021-12-26T12:23:11.417695: Epoch   0 Batch 2840/3125   train_loss = 1.135\n",
      "2021-12-26T12:23:12.171074: Epoch   0 Batch 2860/3125   train_loss = 1.134\n",
      "2021-12-26T12:23:12.948059: Epoch   0 Batch 2880/3125   train_loss = 1.206\n",
      "2021-12-26T12:23:13.690431: Epoch   0 Batch 2900/3125   train_loss = 1.124\n",
      "2021-12-26T12:23:14.467877: Epoch   0 Batch 2920/3125   train_loss = 1.206\n",
      "2021-12-26T12:23:15.229932: Epoch   0 Batch 2940/3125   train_loss = 1.135\n",
      "2021-12-26T12:23:16.018512: Epoch   0 Batch 2960/3125   train_loss = 1.203\n",
      "2021-12-26T12:23:16.758363: Epoch   0 Batch 2980/3125   train_loss = 1.218\n",
      "2021-12-26T12:23:17.525112: Epoch   0 Batch 3000/3125   train_loss = 1.131\n",
      "2021-12-26T12:23:18.224414: Epoch   0 Batch 3020/3125   train_loss = 1.248\n",
      "2021-12-26T12:23:18.956928: Epoch   0 Batch 3040/3125   train_loss = 1.172\n",
      "2021-12-26T12:23:19.743545: Epoch   0 Batch 3060/3125   train_loss = 1.072\n",
      "2021-12-26T12:23:20.498346: Epoch   0 Batch 3080/3125   train_loss = 1.220\n",
      "2021-12-26T12:23:21.248539: Epoch   0 Batch 3100/3125   train_loss = 1.208\n",
      "2021-12-26T12:23:22.016276: Epoch   0 Batch 3120/3125   train_loss = 1.047\n",
      "2021-12-26T12:23:22.293789: Epoch   0 Batch    0/781   test_loss = 0.979\n",
      "2021-12-26T12:23:22.557751: Epoch   0 Batch   20/781   test_loss = 1.092\n",
      "2021-12-26T12:23:22.797870: Epoch   0 Batch   40/781   test_loss = 1.074\n",
      "2021-12-26T12:23:23.033153: Epoch   0 Batch   60/781   test_loss = 1.302\n",
      "2021-12-26T12:23:23.261801: Epoch   0 Batch   80/781   test_loss = 1.305\n",
      "2021-12-26T12:23:23.507433: Epoch   0 Batch  100/781   test_loss = 1.347\n",
      "2021-12-26T12:23:23.753434: Epoch   0 Batch  120/781   test_loss = 1.221\n",
      "2021-12-26T12:23:23.989842: Epoch   0 Batch  140/781   test_loss = 1.187\n",
      "2021-12-26T12:23:24.215686: Epoch   0 Batch  160/781   test_loss = 1.306\n",
      "2021-12-26T12:23:24.437443: Epoch   0 Batch  180/781   test_loss = 1.191\n",
      "2021-12-26T12:23:24.700977: Epoch   0 Batch  200/781   test_loss = 1.101\n",
      "2021-12-26T12:23:24.940348: Epoch   0 Batch  220/781   test_loss = 0.996\n",
      "2021-12-26T12:23:25.194508: Epoch   0 Batch  240/781   test_loss = 1.087\n",
      "2021-12-26T12:23:25.450622: Epoch   0 Batch  260/781   test_loss = 1.130\n",
      "2021-12-26T12:23:25.664966: Epoch   0 Batch  280/781   test_loss = 1.396\n",
      "2021-12-26T12:23:25.894924: Epoch   0 Batch  300/781   test_loss = 1.204\n",
      "2021-12-26T12:23:26.146529: Epoch   0 Batch  320/781   test_loss = 1.287\n",
      "2021-12-26T12:23:26.366173: Epoch   0 Batch  340/781   test_loss = 0.871\n",
      "2021-12-26T12:23:26.616887: Epoch   0 Batch  360/781   test_loss = 1.248\n",
      "2021-12-26T12:23:26.819910: Epoch   0 Batch  380/781   test_loss = 1.141\n",
      "2021-12-26T12:23:27.057971: Epoch   0 Batch  400/781   test_loss = 1.079\n",
      "2021-12-26T12:23:27.282624: Epoch   0 Batch  420/781   test_loss = 1.038\n",
      "2021-12-26T12:23:27.527406: Epoch   0 Batch  440/781   test_loss = 1.220\n",
      "2021-12-26T12:23:28.116546: Epoch   0 Batch  460/781   test_loss = 1.065\n",
      "2021-12-26T12:23:28.630939: Epoch   0 Batch  480/781   test_loss = 1.066\n",
      "2021-12-26T12:23:28.952533: Epoch   0 Batch  500/781   test_loss = 0.991\n",
      "2021-12-26T12:23:29.178972: Epoch   0 Batch  520/781   test_loss = 1.212\n",
      "2021-12-26T12:23:29.747935: Epoch   0 Batch  540/781   test_loss = 0.976\n",
      "2021-12-26T12:23:30.275042: Epoch   0 Batch  560/781   test_loss = 1.275\n",
      "2021-12-26T12:23:30.605490: Epoch   0 Batch  580/781   test_loss = 1.134\n",
      "2021-12-26T12:23:30.842525: Epoch   0 Batch  600/781   test_loss = 1.164\n",
      "2021-12-26T12:23:31.085354: Epoch   0 Batch  620/781   test_loss = 1.186\n",
      "2021-12-26T12:23:31.318033: Epoch   0 Batch  640/781   test_loss = 1.250\n",
      "2021-12-26T12:23:31.555946: Epoch   0 Batch  660/781   test_loss = 1.134\n",
      "2021-12-26T12:23:31.803653: Epoch   0 Batch  680/781   test_loss = 1.332\n",
      "2021-12-26T12:23:32.029250: Epoch   0 Batch  700/781   test_loss = 1.164\n",
      "2021-12-26T12:23:32.267392: Epoch   0 Batch  720/781   test_loss = 1.316\n",
      "2021-12-26T12:23:32.496852: Epoch   0 Batch  740/781   test_loss = 1.160\n",
      "2021-12-26T12:23:32.747708: Epoch   0 Batch  760/781   test_loss = 1.148\n",
      "2021-12-26T12:23:33.015102: Epoch   0 Batch  780/781   test_loss = 1.172\n",
      "2021-12-26T12:23:34.563224: Epoch   1 Batch   15/3125   train_loss = 1.261\n",
      "2021-12-26T12:23:35.336356: Epoch   1 Batch   35/3125   train_loss = 1.166\n",
      "2021-12-26T12:23:36.095607: Epoch   1 Batch   55/3125   train_loss = 1.207\n",
      "2021-12-26T12:23:36.861606: Epoch   1 Batch   75/3125   train_loss = 1.116\n",
      "2021-12-26T12:23:37.607000: Epoch   1 Batch   95/3125   train_loss = 1.038\n",
      "2021-12-26T12:23:38.375189: Epoch   1 Batch  115/3125   train_loss = 1.186\n",
      "2021-12-26T12:23:39.136504: Epoch   1 Batch  135/3125   train_loss = 1.025\n",
      "2021-12-26T12:23:39.912842: Epoch   1 Batch  155/3125   train_loss = 1.160\n",
      "2021-12-26T12:23:40.673021: Epoch   1 Batch  175/3125   train_loss = 1.081\n",
      "2021-12-26T12:23:41.418250: Epoch   1 Batch  195/3125   train_loss = 1.318\n",
      "2021-12-26T12:23:42.186072: Epoch   1 Batch  215/3125   train_loss = 1.122\n",
      "2021-12-26T12:23:42.926947: Epoch   1 Batch  235/3125   train_loss = 1.046\n",
      "2021-12-26T12:23:43.660204: Epoch   1 Batch  255/3125   train_loss = 1.258\n",
      "2021-12-26T12:23:44.416966: Epoch   1 Batch  275/3125   train_loss = 1.081\n",
      "2021-12-26T12:23:45.150817: Epoch   1 Batch  295/3125   train_loss = 1.005\n",
      "2021-12-26T12:23:45.873824: Epoch   1 Batch  315/3125   train_loss = 1.060\n",
      "2021-12-26T12:23:46.610071: Epoch   1 Batch  335/3125   train_loss = 0.983\n",
      "2021-12-26T12:23:47.376961: Epoch   1 Batch  355/3125   train_loss = 1.154\n",
      "2021-12-26T12:23:48.182778: Epoch   1 Batch  375/3125   train_loss = 1.180\n",
      "2021-12-26T12:23:48.928283: Epoch   1 Batch  395/3125   train_loss = 1.046\n",
      "2021-12-26T12:23:49.703026: Epoch   1 Batch  415/3125   train_loss = 1.349\n",
      "2021-12-26T12:23:50.480511: Epoch   1 Batch  435/3125   train_loss = 1.122\n",
      "2021-12-26T12:23:51.268313: Epoch   1 Batch  455/3125   train_loss = 1.092\n",
      "2021-12-26T12:23:51.992633: Epoch   1 Batch  475/3125   train_loss = 1.152\n",
      "2021-12-26T12:23:52.758064: Epoch   1 Batch  495/3125   train_loss = 1.099\n",
      "2021-12-26T12:23:53.546440: Epoch   1 Batch  515/3125   train_loss = 1.187\n",
      "2021-12-26T12:23:54.287157: Epoch   1 Batch  535/3125   train_loss = 1.133\n",
      "2021-12-26T12:23:55.066367: Epoch   1 Batch  555/3125   train_loss = 1.293\n",
      "2021-12-26T12:23:55.804220: Epoch   1 Batch  575/3125   train_loss = 1.033\n",
      "2021-12-26T12:23:56.557187: Epoch   1 Batch  595/3125   train_loss = 1.295\n",
      "2021-12-26T12:23:57.320881: Epoch   1 Batch  615/3125   train_loss = 1.052\n",
      "2021-12-26T12:23:58.081831: Epoch   1 Batch  635/3125   train_loss = 1.101\n",
      "2021-12-26T12:23:59.462800: Epoch   1 Batch  655/3125   train_loss = 1.079\n",
      "2021-12-26T12:24:00.326124: Epoch   1 Batch  675/3125   train_loss = 0.975\n",
      "2021-12-26T12:24:01.087815: Epoch   1 Batch  695/3125   train_loss = 1.050\n",
      "2021-12-26T12:24:01.822845: Epoch   1 Batch  715/3125   train_loss = 1.126\n",
      "2021-12-26T12:24:02.556132: Epoch   1 Batch  735/3125   train_loss = 0.990\n",
      "2021-12-26T12:24:03.288865: Epoch   1 Batch  755/3125   train_loss = 1.143\n",
      "2021-12-26T12:24:04.068669: Epoch   1 Batch  775/3125   train_loss = 1.019\n",
      "2021-12-26T12:24:04.837120: Epoch   1 Batch  795/3125   train_loss = 1.242\n",
      "2021-12-26T12:24:05.599426: Epoch   1 Batch  815/3125   train_loss = 1.122\n",
      "2021-12-26T12:24:06.358767: Epoch   1 Batch  835/3125   train_loss = 1.073\n",
      "2021-12-26T12:24:07.156073: Epoch   1 Batch  855/3125   train_loss = 1.342\n",
      "2021-12-26T12:24:07.905389: Epoch   1 Batch  875/3125   train_loss = 1.171\n",
      "2021-12-26T12:24:08.672330: Epoch   1 Batch  895/3125   train_loss = 1.029\n",
      "2021-12-26T12:24:09.405198: Epoch   1 Batch  915/3125   train_loss = 1.155\n",
      "2021-12-26T12:24:10.174654: Epoch   1 Batch  935/3125   train_loss = 1.261\n",
      "2021-12-26T12:24:10.923749: Epoch   1 Batch  955/3125   train_loss = 1.153\n",
      "2021-12-26T12:24:11.668831: Epoch   1 Batch  975/3125   train_loss = 1.133\n",
      "2021-12-26T12:24:12.387337: Epoch   1 Batch  995/3125   train_loss = 0.883\n",
      "2021-12-26T12:24:13.130058: Epoch   1 Batch 1015/3125   train_loss = 1.135\n",
      "2021-12-26T12:24:13.920294: Epoch   1 Batch 1035/3125   train_loss = 1.013\n",
      "2021-12-26T12:24:14.676845: Epoch   1 Batch 1055/3125   train_loss = 1.126\n",
      "2021-12-26T12:24:15.412167: Epoch   1 Batch 1075/3125   train_loss = 1.055\n",
      "2021-12-26T12:24:16.194817: Epoch   1 Batch 1095/3125   train_loss = 0.975\n",
      "2021-12-26T12:24:16.949488: Epoch   1 Batch 1115/3125   train_loss = 1.162\n",
      "2021-12-26T12:24:17.715270: Epoch   1 Batch 1135/3125   train_loss = 1.119\n",
      "2021-12-26T12:24:18.454021: Epoch   1 Batch 1155/3125   train_loss = 1.100\n",
      "2021-12-26T12:24:19.218004: Epoch   1 Batch 1175/3125   train_loss = 1.074\n",
      "2021-12-26T12:24:19.964529: Epoch   1 Batch 1195/3125   train_loss = 1.249\n",
      "2021-12-26T12:24:20.733032: Epoch   1 Batch 1215/3125   train_loss = 0.941\n",
      "2021-12-26T12:24:21.504955: Epoch   1 Batch 1235/3125   train_loss = 1.083\n",
      "2021-12-26T12:24:22.235347: Epoch   1 Batch 1255/3125   train_loss = 0.964\n",
      "2021-12-26T12:24:22.973298: Epoch   1 Batch 1275/3125   train_loss = 1.017\n",
      "2021-12-26T12:24:23.717204: Epoch   1 Batch 1295/3125   train_loss = 1.039\n",
      "2021-12-26T12:24:24.467664: Epoch   1 Batch 1315/3125   train_loss = 1.256\n",
      "2021-12-26T12:24:25.260920: Epoch   1 Batch 1335/3125   train_loss = 1.034\n",
      "2021-12-26T12:24:26.008449: Epoch   1 Batch 1355/3125   train_loss = 1.059\n",
      "2021-12-26T12:24:26.744782: Epoch   1 Batch 1375/3125   train_loss = 1.203\n",
      "2021-12-26T12:24:27.602902: Epoch   1 Batch 1395/3125   train_loss = 1.025\n",
      "2021-12-26T12:24:28.395163: Epoch   1 Batch 1415/3125   train_loss = 1.077\n",
      "2021-12-26T12:24:29.183316: Epoch   1 Batch 1435/3125   train_loss = 1.160\n",
      "2021-12-26T12:24:29.956324: Epoch   1 Batch 1455/3125   train_loss = 1.174\n",
      "2021-12-26T12:24:30.689897: Epoch   1 Batch 1475/3125   train_loss = 1.148\n",
      "2021-12-26T12:24:31.435161: Epoch   1 Batch 1495/3125   train_loss = 1.061\n",
      "2021-12-26T12:24:32.169346: Epoch   1 Batch 1515/3125   train_loss = 1.033\n",
      "2021-12-26T12:24:32.929357: Epoch   1 Batch 1535/3125   train_loss = 0.915\n",
      "2021-12-26T12:24:33.690664: Epoch   1 Batch 1555/3125   train_loss = 1.057\n",
      "2021-12-26T12:24:34.427998: Epoch   1 Batch 1575/3125   train_loss = 1.080\n",
      "2021-12-26T12:24:35.250209: Epoch   1 Batch 1595/3125   train_loss = 1.167\n",
      "2021-12-26T12:24:35.964790: Epoch   1 Batch 1615/3125   train_loss = 0.998\n",
      "2021-12-26T12:24:36.716124: Epoch   1 Batch 1635/3125   train_loss = 1.133\n",
      "2021-12-26T12:24:37.481422: Epoch   1 Batch 1655/3125   train_loss = 1.105\n",
      "2021-12-26T12:24:38.226773: Epoch   1 Batch 1675/3125   train_loss = 1.010\n",
      "2021-12-26T12:24:38.950697: Epoch   1 Batch 1695/3125   train_loss = 1.077\n",
      "2021-12-26T12:24:39.720268: Epoch   1 Batch 1715/3125   train_loss = 0.983\n",
      "2021-12-26T12:24:40.455457: Epoch   1 Batch 1735/3125   train_loss = 1.190\n",
      "2021-12-26T12:24:42.021338: Epoch   1 Batch 1755/3125   train_loss = 1.036\n",
      "2021-12-26T12:24:43.396993: Epoch   1 Batch 1775/3125   train_loss = 1.031\n",
      "2021-12-26T12:24:44.335185: Epoch   1 Batch 1795/3125   train_loss = 1.071\n",
      "2021-12-26T12:24:45.069144: Epoch   1 Batch 1815/3125   train_loss = 1.015\n",
      "2021-12-26T12:24:45.863437: Epoch   1 Batch 1835/3125   train_loss = 1.193\n",
      "2021-12-26T12:24:46.608595: Epoch   1 Batch 1855/3125   train_loss = 0.938\n",
      "2021-12-26T12:24:47.354584: Epoch   1 Batch 1875/3125   train_loss = 1.156\n",
      "2021-12-26T12:24:48.088978: Epoch   1 Batch 1895/3125   train_loss = 0.975\n",
      "2021-12-26T12:24:48.841378: Epoch   1 Batch 1915/3125   train_loss = 0.939\n",
      "2021-12-26T12:24:49.622690: Epoch   1 Batch 1935/3125   train_loss = 1.025\n",
      "2021-12-26T12:24:50.380420: Epoch   1 Batch 1955/3125   train_loss = 0.996\n",
      "2021-12-26T12:24:51.102799: Epoch   1 Batch 1975/3125   train_loss = 1.057\n",
      "2021-12-26T12:24:51.811346: Epoch   1 Batch 1995/3125   train_loss = 1.144\n",
      "2021-12-26T12:24:52.559154: Epoch   1 Batch 2015/3125   train_loss = 1.108\n",
      "2021-12-26T12:24:53.297035: Epoch   1 Batch 2035/3125   train_loss = 1.094\n",
      "2021-12-26T12:24:54.060102: Epoch   1 Batch 2055/3125   train_loss = 1.017\n",
      "2021-12-26T12:24:54.807245: Epoch   1 Batch 2075/3125   train_loss = 1.116\n",
      "2021-12-26T12:24:55.583137: Epoch   1 Batch 2095/3125   train_loss = 0.946\n",
      "2021-12-26T12:24:56.318706: Epoch   1 Batch 2115/3125   train_loss = 1.138\n",
      "2021-12-26T12:24:57.046033: Epoch   1 Batch 2135/3125   train_loss = 1.001\n",
      "2021-12-26T12:24:57.792102: Epoch   1 Batch 2155/3125   train_loss = 0.983\n",
      "2021-12-26T12:24:58.523078: Epoch   1 Batch 2175/3125   train_loss = 1.022\n",
      "2021-12-26T12:24:59.274269: Epoch   1 Batch 2195/3125   train_loss = 1.085\n",
      "2021-12-26T12:25:00.014630: Epoch   1 Batch 2215/3125   train_loss = 1.141\n",
      "2021-12-26T12:25:00.741628: Epoch   1 Batch 2235/3125   train_loss = 1.168\n",
      "2021-12-26T12:25:01.496069: Epoch   1 Batch 2255/3125   train_loss = 1.145\n",
      "2021-12-26T12:25:02.271070: Epoch   1 Batch 2275/3125   train_loss = 0.906\n",
      "2021-12-26T12:25:03.024336: Epoch   1 Batch 2295/3125   train_loss = 1.276\n",
      "2021-12-26T12:25:03.780406: Epoch   1 Batch 2315/3125   train_loss = 1.205\n",
      "2021-12-26T12:25:04.528673: Epoch   1 Batch 2335/3125   train_loss = 1.123\n",
      "2021-12-26T12:25:05.269202: Epoch   1 Batch 2355/3125   train_loss = 1.048\n",
      "2021-12-26T12:25:06.032060: Epoch   1 Batch 2375/3125   train_loss = 1.325\n",
      "2021-12-26T12:25:06.773531: Epoch   1 Batch 2395/3125   train_loss = 1.067\n",
      "2021-12-26T12:25:07.543446: Epoch   1 Batch 2415/3125   train_loss = 1.064\n",
      "2021-12-26T12:25:08.300060: Epoch   1 Batch 2435/3125   train_loss = 0.998\n",
      "2021-12-26T12:25:09.057838: Epoch   1 Batch 2455/3125   train_loss = 1.125\n",
      "2021-12-26T12:25:09.813600: Epoch   1 Batch 2475/3125   train_loss = 1.024\n",
      "2021-12-26T12:25:10.584586: Epoch   1 Batch 2495/3125   train_loss = 1.024\n",
      "2021-12-26T12:25:11.319072: Epoch   1 Batch 2515/3125   train_loss = 1.048\n",
      "2021-12-26T12:25:12.064142: Epoch   1 Batch 2535/3125   train_loss = 1.027\n",
      "2021-12-26T12:25:12.798584: Epoch   1 Batch 2555/3125   train_loss = 0.905\n",
      "2021-12-26T12:25:13.539895: Epoch   1 Batch 2575/3125   train_loss = 0.923\n",
      "2021-12-26T12:25:14.292946: Epoch   1 Batch 2595/3125   train_loss = 0.977\n",
      "2021-12-26T12:25:15.062685: Epoch   1 Batch 2615/3125   train_loss = 1.178\n",
      "2021-12-26T12:25:15.849106: Epoch   1 Batch 2635/3125   train_loss = 0.965\n",
      "2021-12-26T12:25:16.608360: Epoch   1 Batch 2655/3125   train_loss = 0.975\n",
      "2021-12-26T12:25:17.367978: Epoch   1 Batch 2675/3125   train_loss = 0.974\n",
      "2021-12-26T12:25:18.116348: Epoch   1 Batch 2695/3125   train_loss = 1.063\n",
      "2021-12-26T12:25:18.841584: Epoch   1 Batch 2715/3125   train_loss = 0.941\n",
      "2021-12-26T12:25:19.587174: Epoch   1 Batch 2735/3125   train_loss = 0.829\n",
      "2021-12-26T12:25:20.335195: Epoch   1 Batch 2755/3125   train_loss = 1.073\n",
      "2021-12-26T12:25:21.083406: Epoch   1 Batch 2775/3125   train_loss = 1.089\n",
      "2021-12-26T12:25:21.810222: Epoch   1 Batch 2795/3125   train_loss = 1.073\n",
      "2021-12-26T12:25:22.571603: Epoch   1 Batch 2815/3125   train_loss = 0.989\n",
      "2021-12-26T12:25:23.305882: Epoch   1 Batch 2835/3125   train_loss = 1.114\n",
      "2021-12-26T12:25:24.040693: Epoch   1 Batch 2855/3125   train_loss = 1.077\n",
      "2021-12-26T12:25:24.787642: Epoch   1 Batch 2875/3125   train_loss = 1.023\n",
      "2021-12-26T12:25:25.549713: Epoch   1 Batch 2895/3125   train_loss = 1.062\n",
      "2021-12-26T12:25:26.307102: Epoch   1 Batch 2915/3125   train_loss = 0.930\n",
      "2021-12-26T12:25:27.053579: Epoch   1 Batch 2935/3125   train_loss = 1.098\n",
      "2021-12-26T12:25:27.817731: Epoch   1 Batch 2955/3125   train_loss = 1.221\n",
      "2021-12-26T12:25:28.571914: Epoch   1 Batch 2975/3125   train_loss = 1.016\n",
      "2021-12-26T12:25:29.339939: Epoch   1 Batch 2995/3125   train_loss = 0.973\n",
      "2021-12-26T12:25:30.087792: Epoch   1 Batch 3015/3125   train_loss = 1.064\n",
      "2021-12-26T12:25:30.846739: Epoch   1 Batch 3035/3125   train_loss = 1.149\n",
      "2021-12-26T12:25:31.653093: Epoch   1 Batch 3055/3125   train_loss = 1.085\n",
      "2021-12-26T12:25:32.415349: Epoch   1 Batch 3075/3125   train_loss = 0.931\n",
      "2021-12-26T12:25:33.193301: Epoch   1 Batch 3095/3125   train_loss = 1.048\n",
      "2021-12-26T12:25:33.937433: Epoch   1 Batch 3115/3125   train_loss = 0.895\n",
      "2021-12-26T12:25:34.491715: Epoch   1 Batch   19/781   test_loss = 1.057\n",
      "2021-12-26T12:25:34.734933: Epoch   1 Batch   39/781   test_loss = 0.814\n",
      "2021-12-26T12:25:34.968352: Epoch   1 Batch   59/781   test_loss = 0.976\n",
      "2021-12-26T12:25:35.184432: Epoch   1 Batch   79/781   test_loss = 1.082\n",
      "2021-12-26T12:25:35.419743: Epoch   1 Batch   99/781   test_loss = 1.005\n",
      "2021-12-26T12:25:35.656917: Epoch   1 Batch  119/781   test_loss = 0.955\n",
      "2021-12-26T12:25:35.896225: Epoch   1 Batch  139/781   test_loss = 0.978\n",
      "2021-12-26T12:25:36.140885: Epoch   1 Batch  159/781   test_loss = 0.988\n",
      "2021-12-26T12:25:36.379079: Epoch   1 Batch  179/781   test_loss = 0.924\n",
      "2021-12-26T12:25:36.622644: Epoch   1 Batch  199/781   test_loss = 0.929\n",
      "2021-12-26T12:25:36.852923: Epoch   1 Batch  219/781   test_loss = 1.061\n",
      "2021-12-26T12:25:37.087439: Epoch   1 Batch  239/781   test_loss = 1.186\n",
      "2021-12-26T12:25:37.301272: Epoch   1 Batch  259/781   test_loss = 0.987\n",
      "2021-12-26T12:25:37.538341: Epoch   1 Batch  279/781   test_loss = 1.102\n",
      "2021-12-26T12:25:37.782864: Epoch   1 Batch  299/781   test_loss = 1.204\n",
      "2021-12-26T12:25:38.023057: Epoch   1 Batch  319/781   test_loss = 1.005\n",
      "2021-12-26T12:25:38.247030: Epoch   1 Batch  339/781   test_loss = 0.974\n",
      "2021-12-26T12:25:38.473737: Epoch   1 Batch  359/781   test_loss = 0.939\n",
      "2021-12-26T12:25:38.712676: Epoch   1 Batch  379/781   test_loss = 1.098\n",
      "2021-12-26T12:25:38.933660: Epoch   1 Batch  399/781   test_loss = 0.839\n",
      "2021-12-26T12:25:39.160919: Epoch   1 Batch  419/781   test_loss = 0.894\n",
      "2021-12-26T12:25:39.409517: Epoch   1 Batch  439/781   test_loss = 1.087\n",
      "2021-12-26T12:25:39.648900: Epoch   1 Batch  459/781   test_loss = 1.082\n",
      "2021-12-26T12:25:39.870449: Epoch   1 Batch  479/781   test_loss = 1.035\n",
      "2021-12-26T12:25:40.122618: Epoch   1 Batch  499/781   test_loss = 0.992\n",
      "2021-12-26T12:25:40.347012: Epoch   1 Batch  519/781   test_loss = 1.089\n",
      "2021-12-26T12:25:40.605550: Epoch   1 Batch  539/781   test_loss = 0.929\n",
      "2021-12-26T12:25:40.833514: Epoch   1 Batch  559/781   test_loss = 1.100\n",
      "2021-12-26T12:25:41.063436: Epoch   1 Batch  579/781   test_loss = 1.010\n",
      "2021-12-26T12:25:41.285149: Epoch   1 Batch  599/781   test_loss = 0.933\n",
      "2021-12-26T12:25:41.517339: Epoch   1 Batch  619/781   test_loss = 1.154\n",
      "2021-12-26T12:25:41.776161: Epoch   1 Batch  639/781   test_loss = 0.918\n",
      "2021-12-26T12:25:42.024419: Epoch   1 Batch  659/781   test_loss = 1.141\n",
      "2021-12-26T12:25:42.280058: Epoch   1 Batch  679/781   test_loss = 1.108\n",
      "2021-12-26T12:25:42.509603: Epoch   1 Batch  699/781   test_loss = 0.860\n",
      "2021-12-26T12:25:42.748303: Epoch   1 Batch  719/781   test_loss = 1.032\n",
      "2021-12-26T12:25:42.993023: Epoch   1 Batch  739/781   test_loss = 0.957\n",
      "2021-12-26T12:25:43.227471: Epoch   1 Batch  759/781   test_loss = 0.938\n",
      "2021-12-26T12:25:43.463928: Epoch   1 Batch  779/781   test_loss = 0.808\n",
      "2021-12-26T12:25:44.861269: Epoch   2 Batch   10/3125   train_loss = 0.922\n",
      "2021-12-26T12:25:45.600110: Epoch   2 Batch   30/3125   train_loss = 1.101\n",
      "2021-12-26T12:25:46.356099: Epoch   2 Batch   50/3125   train_loss = 1.047\n",
      "2021-12-26T12:25:47.121438: Epoch   2 Batch   70/3125   train_loss = 0.998\n",
      "2021-12-26T12:25:47.840576: Epoch   2 Batch   90/3125   train_loss = 1.055\n",
      "2021-12-26T12:25:48.611431: Epoch   2 Batch  110/3125   train_loss = 0.896\n",
      "2021-12-26T12:25:49.400896: Epoch   2 Batch  130/3125   train_loss = 1.005\n",
      "2021-12-26T12:25:50.163812: Epoch   2 Batch  150/3125   train_loss = 1.156\n",
      "2021-12-26T12:25:50.939150: Epoch   2 Batch  170/3125   train_loss = 0.977\n",
      "2021-12-26T12:25:51.692736: Epoch   2 Batch  190/3125   train_loss = 1.003\n",
      "2021-12-26T12:25:52.453739: Epoch   2 Batch  210/3125   train_loss = 0.913\n",
      "2021-12-26T12:25:53.230104: Epoch   2 Batch  230/3125   train_loss = 1.038\n",
      "2021-12-26T12:25:53.992080: Epoch   2 Batch  250/3125   train_loss = 0.987\n",
      "2021-12-26T12:25:54.735938: Epoch   2 Batch  270/3125   train_loss = 0.821\n",
      "2021-12-26T12:25:55.486805: Epoch   2 Batch  290/3125   train_loss = 1.192\n",
      "2021-12-26T12:25:56.231497: Epoch   2 Batch  310/3125   train_loss = 1.019\n",
      "2021-12-26T12:25:57.010430: Epoch   2 Batch  330/3125   train_loss = 1.043\n",
      "2021-12-26T12:25:57.776119: Epoch   2 Batch  350/3125   train_loss = 0.951\n",
      "2021-12-26T12:25:58.530189: Epoch   2 Batch  370/3125   train_loss = 1.083\n",
      "2021-12-26T12:25:59.328001: Epoch   2 Batch  390/3125   train_loss = 1.210\n",
      "2021-12-26T12:26:00.117741: Epoch   2 Batch  410/3125   train_loss = 0.918\n",
      "2021-12-26T12:26:00.873196: Epoch   2 Batch  430/3125   train_loss = 1.123\n",
      "2021-12-26T12:26:01.636136: Epoch   2 Batch  450/3125   train_loss = 1.004\n",
      "2021-12-26T12:26:02.412415: Epoch   2 Batch  470/3125   train_loss = 0.970\n",
      "2021-12-26T12:26:03.185177: Epoch   2 Batch  490/3125   train_loss = 0.977\n",
      "2021-12-26T12:26:03.949389: Epoch   2 Batch  510/3125   train_loss = 1.066\n",
      "2021-12-26T12:26:04.735380: Epoch   2 Batch  530/3125   train_loss = 0.945\n",
      "2021-12-26T12:26:05.514021: Epoch   2 Batch  550/3125   train_loss = 1.039\n",
      "2021-12-26T12:26:06.287520: Epoch   2 Batch  570/3125   train_loss = 1.119\n",
      "2021-12-26T12:26:07.069146: Epoch   2 Batch  590/3125   train_loss = 1.009\n",
      "2021-12-26T12:26:07.849122: Epoch   2 Batch  610/3125   train_loss = 1.010\n",
      "2021-12-26T12:26:08.614095: Epoch   2 Batch  630/3125   train_loss = 1.040\n",
      "2021-12-26T12:26:09.416094: Epoch   2 Batch  650/3125   train_loss = 0.997\n",
      "2021-12-26T12:26:10.174224: Epoch   2 Batch  670/3125   train_loss = 0.958\n",
      "2021-12-26T12:26:10.945788: Epoch   2 Batch  690/3125   train_loss = 0.951\n",
      "2021-12-26T12:26:11.721781: Epoch   2 Batch  710/3125   train_loss = 0.940\n",
      "2021-12-26T12:26:12.472677: Epoch   2 Batch  730/3125   train_loss = 0.873\n",
      "2021-12-26T12:26:13.233701: Epoch   2 Batch  750/3125   train_loss = 0.951\n",
      "2021-12-26T12:26:14.030336: Epoch   2 Batch  770/3125   train_loss = 0.875\n",
      "2021-12-26T12:26:14.769703: Epoch   2 Batch  790/3125   train_loss = 0.882\n",
      "2021-12-26T12:26:15.524653: Epoch   2 Batch  810/3125   train_loss = 0.821\n",
      "2021-12-26T12:26:16.256366: Epoch   2 Batch  830/3125   train_loss = 0.846\n",
      "2021-12-26T12:26:17.009212: Epoch   2 Batch  850/3125   train_loss = 1.008\n",
      "2021-12-26T12:26:17.763926: Epoch   2 Batch  870/3125   train_loss = 0.934\n",
      "2021-12-26T12:26:18.521772: Epoch   2 Batch  890/3125   train_loss = 0.941\n",
      "2021-12-26T12:26:19.296429: Epoch   2 Batch  910/3125   train_loss = 0.974\n",
      "2021-12-26T12:26:20.094047: Epoch   2 Batch  930/3125   train_loss = 1.049\n",
      "2021-12-26T12:26:20.832294: Epoch   2 Batch  950/3125   train_loss = 0.962\n",
      "2021-12-26T12:26:21.577944: Epoch   2 Batch  970/3125   train_loss = 1.052\n",
      "2021-12-26T12:26:22.329281: Epoch   2 Batch  990/3125   train_loss = 0.878\n",
      "2021-12-26T12:26:23.069173: Epoch   2 Batch 1010/3125   train_loss = 1.061\n",
      "2021-12-26T12:26:23.820842: Epoch   2 Batch 1030/3125   train_loss = 0.944\n",
      "2021-12-26T12:26:24.588138: Epoch   2 Batch 1050/3125   train_loss = 1.013\n",
      "2021-12-26T12:26:25.313179: Epoch   2 Batch 1070/3125   train_loss = 0.977\n",
      "2021-12-26T12:26:26.033679: Epoch   2 Batch 1090/3125   train_loss = 1.011\n",
      "2021-12-26T12:26:26.802986: Epoch   2 Batch 1110/3125   train_loss = 1.111\n",
      "2021-12-26T12:26:27.584255: Epoch   2 Batch 1130/3125   train_loss = 0.964\n",
      "2021-12-26T12:26:28.313302: Epoch   2 Batch 1150/3125   train_loss = 0.991\n",
      "2021-12-26T12:26:29.101361: Epoch   2 Batch 1170/3125   train_loss = 0.960\n",
      "2021-12-26T12:26:29.834808: Epoch   2 Batch 1190/3125   train_loss = 1.038\n",
      "2021-12-26T12:26:30.572142: Epoch   2 Batch 1210/3125   train_loss = 0.933\n",
      "2021-12-26T12:26:31.332290: Epoch   2 Batch 1230/3125   train_loss = 0.884\n",
      "2021-12-26T12:26:32.076827: Epoch   2 Batch 1250/3125   train_loss = 1.006\n",
      "2021-12-26T12:26:32.846895: Epoch   2 Batch 1270/3125   train_loss = 0.992\n",
      "2021-12-26T12:26:33.601241: Epoch   2 Batch 1290/3125   train_loss = 0.951\n",
      "2021-12-26T12:26:34.360725: Epoch   2 Batch 1310/3125   train_loss = 0.973\n",
      "2021-12-26T12:26:35.126074: Epoch   2 Batch 1330/3125   train_loss = 1.057\n",
      "2021-12-26T12:26:35.892385: Epoch   2 Batch 1350/3125   train_loss = 0.945\n",
      "2021-12-26T12:26:36.636279: Epoch   2 Batch 1370/3125   train_loss = 0.851\n",
      "2021-12-26T12:26:37.411690: Epoch   2 Batch 1390/3125   train_loss = 1.038\n",
      "2021-12-26T12:26:38.193280: Epoch   2 Batch 1410/3125   train_loss = 1.006\n",
      "2021-12-26T12:26:38.973336: Epoch   2 Batch 1430/3125   train_loss = 0.948\n",
      "2021-12-26T12:26:39.746233: Epoch   2 Batch 1450/3125   train_loss = 1.015\n",
      "2021-12-26T12:26:40.510264: Epoch   2 Batch 1470/3125   train_loss = 1.032\n",
      "2021-12-26T12:26:41.269309: Epoch   2 Batch 1490/3125   train_loss = 1.043\n",
      "2021-12-26T12:26:42.004848: Epoch   2 Batch 1510/3125   train_loss = 0.985\n",
      "2021-12-26T12:26:42.770370: Epoch   2 Batch 1530/3125   train_loss = 1.083\n",
      "2021-12-26T12:26:43.535723: Epoch   2 Batch 1550/3125   train_loss = 0.883\n",
      "2021-12-26T12:26:44.300865: Epoch   2 Batch 1570/3125   train_loss = 0.962\n",
      "2021-12-26T12:26:45.088207: Epoch   2 Batch 1590/3125   train_loss = 1.036\n",
      "2021-12-26T12:26:45.851887: Epoch   2 Batch 1610/3125   train_loss = 1.035\n",
      "2021-12-26T12:26:46.651258: Epoch   2 Batch 1630/3125   train_loss = 0.961\n",
      "2021-12-26T12:26:47.420089: Epoch   2 Batch 1650/3125   train_loss = 0.850\n",
      "2021-12-26T12:26:48.212697: Epoch   2 Batch 1670/3125   train_loss = 0.860\n",
      "2021-12-26T12:26:48.964273: Epoch   2 Batch 1690/3125   train_loss = 1.076\n",
      "2021-12-26T12:26:49.732896: Epoch   2 Batch 1710/3125   train_loss = 0.937\n",
      "2021-12-26T12:26:50.505446: Epoch   2 Batch 1730/3125   train_loss = 0.976\n",
      "2021-12-26T12:26:51.302236: Epoch   2 Batch 1750/3125   train_loss = 0.862\n",
      "2021-12-26T12:26:52.041477: Epoch   2 Batch 1770/3125   train_loss = 1.167\n",
      "2021-12-26T12:26:52.807879: Epoch   2 Batch 1790/3125   train_loss = 0.999\n",
      "2021-12-26T12:26:53.556356: Epoch   2 Batch 1810/3125   train_loss = 1.025\n",
      "2021-12-26T12:26:54.319928: Epoch   2 Batch 1830/3125   train_loss = 1.002\n",
      "2021-12-26T12:26:55.068575: Epoch   2 Batch 1850/3125   train_loss = 0.910\n",
      "2021-12-26T12:26:55.806132: Epoch   2 Batch 1870/3125   train_loss = 1.028\n",
      "2021-12-26T12:26:56.555012: Epoch   2 Batch 1890/3125   train_loss = 0.829\n",
      "2021-12-26T12:26:57.290846: Epoch   2 Batch 1910/3125   train_loss = 0.895\n",
      "2021-12-26T12:26:58.030031: Epoch   2 Batch 1930/3125   train_loss = 1.032\n",
      "2021-12-26T12:26:58.770903: Epoch   2 Batch 1950/3125   train_loss = 0.846\n",
      "2021-12-26T12:26:59.519945: Epoch   2 Batch 1970/3125   train_loss = 0.999\n",
      "2021-12-26T12:27:00.280609: Epoch   2 Batch 1990/3125   train_loss = 0.836\n",
      "2021-12-26T12:27:01.044356: Epoch   2 Batch 2010/3125   train_loss = 0.855\n",
      "2021-12-26T12:27:01.777029: Epoch   2 Batch 2030/3125   train_loss = 0.914\n",
      "2021-12-26T12:27:02.498151: Epoch   2 Batch 2050/3125   train_loss = 0.965\n",
      "2021-12-26T12:27:03.219507: Epoch   2 Batch 2070/3125   train_loss = 0.946\n",
      "2021-12-26T12:27:03.970511: Epoch   2 Batch 2090/3125   train_loss = 0.850\n",
      "2021-12-26T12:27:04.747501: Epoch   2 Batch 2110/3125   train_loss = 1.025\n",
      "2021-12-26T12:27:05.471227: Epoch   2 Batch 2130/3125   train_loss = 0.886\n",
      "2021-12-26T12:27:06.216368: Epoch   2 Batch 2150/3125   train_loss = 0.972\n",
      "2021-12-26T12:27:07.018510: Epoch   2 Batch 2170/3125   train_loss = 0.879\n",
      "2021-12-26T12:27:07.776961: Epoch   2 Batch 2190/3125   train_loss = 0.949\n",
      "2021-12-26T12:27:08.487547: Epoch   2 Batch 2210/3125   train_loss = 0.974\n",
      "2021-12-26T12:27:09.248961: Epoch   2 Batch 2230/3125   train_loss = 0.916\n",
      "2021-12-26T12:27:10.019043: Epoch   2 Batch 2250/3125   train_loss = 1.064\n",
      "2021-12-26T12:27:10.775236: Epoch   2 Batch 2270/3125   train_loss = 0.906\n",
      "2021-12-26T12:27:11.541580: Epoch   2 Batch 2290/3125   train_loss = 0.883\n",
      "2021-12-26T12:27:12.316767: Epoch   2 Batch 2310/3125   train_loss = 0.925\n",
      "2021-12-26T12:27:13.073565: Epoch   2 Batch 2330/3125   train_loss = 1.095\n",
      "2021-12-26T12:27:13.831380: Epoch   2 Batch 2350/3125   train_loss = 1.060\n",
      "2021-12-26T12:27:14.575536: Epoch   2 Batch 2370/3125   train_loss = 0.973\n",
      "2021-12-26T12:27:15.352212: Epoch   2 Batch 2390/3125   train_loss = 1.012\n",
      "2021-12-26T12:27:16.121392: Epoch   2 Batch 2410/3125   train_loss = 1.104\n",
      "2021-12-26T12:27:16.875720: Epoch   2 Batch 2430/3125   train_loss = 0.913\n",
      "2021-12-26T12:27:17.637532: Epoch   2 Batch 2450/3125   train_loss = 0.991\n",
      "2021-12-26T12:27:18.392491: Epoch   2 Batch 2470/3125   train_loss = 0.991\n",
      "2021-12-26T12:27:19.142812: Epoch   2 Batch 2490/3125   train_loss = 1.031\n",
      "2021-12-26T12:27:19.906365: Epoch   2 Batch 2510/3125   train_loss = 1.093\n",
      "2021-12-26T12:27:20.646167: Epoch   2 Batch 2530/3125   train_loss = 0.855\n",
      "2021-12-26T12:27:21.409503: Epoch   2 Batch 2550/3125   train_loss = 1.025\n",
      "2021-12-26T12:27:22.177379: Epoch   2 Batch 2570/3125   train_loss = 0.987\n",
      "2021-12-26T12:27:22.942685: Epoch   2 Batch 2590/3125   train_loss = 0.901\n",
      "2021-12-26T12:27:23.707251: Epoch   2 Batch 2610/3125   train_loss = 1.029\n",
      "2021-12-26T12:27:24.456647: Epoch   2 Batch 2630/3125   train_loss = 0.673\n",
      "2021-12-26T12:27:25.210103: Epoch   2 Batch 2650/3125   train_loss = 0.906\n",
      "2021-12-26T12:27:25.971847: Epoch   2 Batch 2670/3125   train_loss = 0.977\n",
      "2021-12-26T12:27:26.737804: Epoch   2 Batch 2690/3125   train_loss = 0.982\n",
      "2021-12-26T12:27:27.523202: Epoch   2 Batch 2710/3125   train_loss = 0.866\n",
      "2021-12-26T12:27:28.284978: Epoch   2 Batch 2730/3125   train_loss = 1.114\n",
      "2021-12-26T12:27:29.055754: Epoch   2 Batch 2750/3125   train_loss = 1.028\n",
      "2021-12-26T12:27:29.812830: Epoch   2 Batch 2770/3125   train_loss = 0.905\n",
      "2021-12-26T12:27:30.569820: Epoch   2 Batch 2790/3125   train_loss = 0.890\n",
      "2021-12-26T12:27:31.360693: Epoch   2 Batch 2810/3125   train_loss = 0.942\n",
      "2021-12-26T12:27:32.152663: Epoch   2 Batch 2830/3125   train_loss = 0.808\n",
      "2021-12-26T12:27:32.924846: Epoch   2 Batch 2850/3125   train_loss = 1.010\n",
      "2021-12-26T12:27:33.705172: Epoch   2 Batch 2870/3125   train_loss = 0.883\n",
      "2021-12-26T12:27:34.455488: Epoch   2 Batch 2890/3125   train_loss = 0.830\n",
      "2021-12-26T12:27:35.252348: Epoch   2 Batch 2910/3125   train_loss = 0.982\n",
      "2021-12-26T12:27:36.002357: Epoch   2 Batch 2930/3125   train_loss = 0.761\n",
      "2021-12-26T12:27:36.768841: Epoch   2 Batch 2950/3125   train_loss = 1.102\n",
      "2021-12-26T12:27:37.527671: Epoch   2 Batch 2970/3125   train_loss = 0.949\n",
      "2021-12-26T12:27:38.307303: Epoch   2 Batch 2990/3125   train_loss = 0.867\n",
      "2021-12-26T12:27:39.052191: Epoch   2 Batch 3010/3125   train_loss = 1.013\n",
      "2021-12-26T12:27:39.806042: Epoch   2 Batch 3030/3125   train_loss = 0.962\n",
      "2021-12-26T12:27:40.586429: Epoch   2 Batch 3050/3125   train_loss = 0.960\n",
      "2021-12-26T12:27:41.368951: Epoch   2 Batch 3070/3125   train_loss = 0.889\n",
      "2021-12-26T12:27:42.097309: Epoch   2 Batch 3090/3125   train_loss = 0.828\n",
      "2021-12-26T12:27:42.884368: Epoch   2 Batch 3110/3125   train_loss = 0.784\n",
      "2021-12-26T12:27:43.661513: Epoch   2 Batch   18/781   test_loss = 0.806\n",
      "2021-12-26T12:27:43.905899: Epoch   2 Batch   38/781   test_loss = 0.956\n",
      "2021-12-26T12:27:44.133676: Epoch   2 Batch   58/781   test_loss = 0.923\n",
      "2021-12-26T12:27:44.369609: Epoch   2 Batch   78/781   test_loss = 0.940\n",
      "2021-12-26T12:27:44.617089: Epoch   2 Batch   98/781   test_loss = 0.962\n",
      "2021-12-26T12:27:44.835390: Epoch   2 Batch  118/781   test_loss = 0.897\n",
      "2021-12-26T12:27:45.086885: Epoch   2 Batch  138/781   test_loss = 0.995\n",
      "2021-12-26T12:27:45.318351: Epoch   2 Batch  158/781   test_loss = 0.904\n",
      "2021-12-26T12:27:45.575746: Epoch   2 Batch  178/781   test_loss = 0.795\n",
      "2021-12-26T12:27:45.841839: Epoch   2 Batch  198/781   test_loss = 0.921\n",
      "2021-12-26T12:27:46.059650: Epoch   2 Batch  218/781   test_loss = 1.055\n",
      "2021-12-26T12:27:46.294267: Epoch   2 Batch  238/781   test_loss = 0.889\n",
      "2021-12-26T12:27:46.553386: Epoch   2 Batch  258/781   test_loss = 0.976\n",
      "2021-12-26T12:27:46.788422: Epoch   2 Batch  278/781   test_loss = 1.024\n",
      "2021-12-26T12:27:47.003493: Epoch   2 Batch  298/781   test_loss = 0.878\n",
      "2021-12-26T12:27:47.269214: Epoch   2 Batch  318/781   test_loss = 0.892\n",
      "2021-12-26T12:27:47.507523: Epoch   2 Batch  338/781   test_loss = 0.936\n",
      "2021-12-26T12:27:47.766642: Epoch   2 Batch  358/781   test_loss = 0.931\n",
      "2021-12-26T12:27:48.010634: Epoch   2 Batch  378/781   test_loss = 0.858\n",
      "2021-12-26T12:27:48.231634: Epoch   2 Batch  398/781   test_loss = 0.844\n",
      "2021-12-26T12:27:48.479220: Epoch   2 Batch  418/781   test_loss = 0.993\n",
      "2021-12-26T12:27:48.690561: Epoch   2 Batch  438/781   test_loss = 1.019\n",
      "2021-12-26T12:27:48.924744: Epoch   2 Batch  458/781   test_loss = 0.891\n",
      "2021-12-26T12:27:49.149084: Epoch   2 Batch  478/781   test_loss = 0.952\n",
      "2021-12-26T12:27:49.357313: Epoch   2 Batch  498/781   test_loss = 0.814\n",
      "2021-12-26T12:27:49.590494: Epoch   2 Batch  518/781   test_loss = 0.908\n",
      "2021-12-26T12:27:49.828605: Epoch   2 Batch  538/781   test_loss = 0.793\n",
      "2021-12-26T12:27:50.067099: Epoch   2 Batch  558/781   test_loss = 0.923\n",
      "2021-12-26T12:27:50.301861: Epoch   2 Batch  578/781   test_loss = 0.959\n",
      "2021-12-26T12:27:50.540440: Epoch   2 Batch  598/781   test_loss = 1.063\n",
      "2021-12-26T12:27:50.787812: Epoch   2 Batch  618/781   test_loss = 0.859\n",
      "2021-12-26T12:27:51.024081: Epoch   2 Batch  638/781   test_loss = 0.888\n",
      "2021-12-26T12:27:51.259767: Epoch   2 Batch  658/781   test_loss = 1.030\n",
      "2021-12-26T12:27:51.500242: Epoch   2 Batch  678/781   test_loss = 0.955\n",
      "2021-12-26T12:27:51.732653: Epoch   2 Batch  698/781   test_loss = 0.852\n",
      "2021-12-26T12:27:51.990381: Epoch   2 Batch  718/781   test_loss = 1.020\n",
      "2021-12-26T12:27:52.254925: Epoch   2 Batch  738/781   test_loss = 0.856\n",
      "2021-12-26T12:27:52.522011: Epoch   2 Batch  758/781   test_loss = 0.977\n",
      "2021-12-26T12:27:52.762649: Epoch   2 Batch  778/781   test_loss = 0.917\n",
      "2021-12-26T12:27:53.999426: Epoch   3 Batch    5/3125   train_loss = 0.930\n",
      "2021-12-26T12:27:54.756429: Epoch   3 Batch   25/3125   train_loss = 1.014\n",
      "2021-12-26T12:27:55.511239: Epoch   3 Batch   45/3125   train_loss = 0.880\n",
      "2021-12-26T12:27:56.284152: Epoch   3 Batch   65/3125   train_loss = 1.006\n",
      "2021-12-26T12:27:57.054679: Epoch   3 Batch   85/3125   train_loss = 0.878\n",
      "2021-12-26T12:27:57.816951: Epoch   3 Batch  105/3125   train_loss = 0.704\n",
      "2021-12-26T12:27:58.567132: Epoch   3 Batch  125/3125   train_loss = 0.949\n",
      "2021-12-26T12:27:59.335034: Epoch   3 Batch  145/3125   train_loss = 0.957\n",
      "2021-12-26T12:28:00.084096: Epoch   3 Batch  165/3125   train_loss = 0.934\n",
      "2021-12-26T12:28:00.837561: Epoch   3 Batch  185/3125   train_loss = 0.836\n",
      "2021-12-26T12:28:01.581467: Epoch   3 Batch  205/3125   train_loss = 0.902\n",
      "2021-12-26T12:28:02.347402: Epoch   3 Batch  225/3125   train_loss = 0.781\n",
      "2021-12-26T12:28:03.085697: Epoch   3 Batch  245/3125   train_loss = 1.084\n",
      "2021-12-26T12:28:03.871252: Epoch   3 Batch  265/3125   train_loss = 0.919\n",
      "2021-12-26T12:28:04.633171: Epoch   3 Batch  285/3125   train_loss = 0.907\n",
      "2021-12-26T12:28:05.406529: Epoch   3 Batch  305/3125   train_loss = 0.871\n",
      "2021-12-26T12:28:06.195982: Epoch   3 Batch  325/3125   train_loss = 0.954\n",
      "2021-12-26T12:28:06.984472: Epoch   3 Batch  345/3125   train_loss = 0.967\n",
      "2021-12-26T12:28:07.759913: Epoch   3 Batch  365/3125   train_loss = 0.859\n",
      "2021-12-26T12:28:08.533821: Epoch   3 Batch  385/3125   train_loss = 0.826\n",
      "2021-12-26T12:28:09.298150: Epoch   3 Batch  405/3125   train_loss = 0.829\n",
      "2021-12-26T12:28:10.046804: Epoch   3 Batch  425/3125   train_loss = 1.022\n",
      "2021-12-26T12:28:10.773146: Epoch   3 Batch  445/3125   train_loss = 0.958\n",
      "2021-12-26T12:28:11.530500: Epoch   3 Batch  465/3125   train_loss = 0.961\n",
      "2021-12-26T12:28:12.298120: Epoch   3 Batch  485/3125   train_loss = 0.957\n",
      "2021-12-26T12:28:13.041904: Epoch   3 Batch  505/3125   train_loss = 0.834\n",
      "2021-12-26T12:28:13.772682: Epoch   3 Batch  525/3125   train_loss = 0.966\n",
      "2021-12-26T12:28:14.567765: Epoch   3 Batch  545/3125   train_loss = 0.875\n",
      "2021-12-26T12:28:15.355965: Epoch   3 Batch  565/3125   train_loss = 1.089\n",
      "2021-12-26T12:28:16.111081: Epoch   3 Batch  585/3125   train_loss = 0.864\n",
      "2021-12-26T12:28:16.887579: Epoch   3 Batch  605/3125   train_loss = 0.912\n",
      "2021-12-26T12:28:17.643028: Epoch   3 Batch  625/3125   train_loss = 0.931\n",
      "2021-12-26T12:28:18.402930: Epoch   3 Batch  645/3125   train_loss = 0.969\n",
      "2021-12-26T12:28:19.151354: Epoch   3 Batch  665/3125   train_loss = 0.997\n",
      "2021-12-26T12:28:19.889436: Epoch   3 Batch  685/3125   train_loss = 0.882\n",
      "2021-12-26T12:28:20.628257: Epoch   3 Batch  705/3125   train_loss = 1.074\n",
      "2021-12-26T12:28:21.386108: Epoch   3 Batch  725/3125   train_loss = 0.864\n",
      "2021-12-26T12:28:22.136969: Epoch   3 Batch  745/3125   train_loss = 0.927\n",
      "2021-12-26T12:28:22.886773: Epoch   3 Batch  765/3125   train_loss = 0.835\n",
      "2021-12-26T12:28:23.648493: Epoch   3 Batch  785/3125   train_loss = 1.121\n",
      "2021-12-26T12:28:24.459725: Epoch   3 Batch  805/3125   train_loss = 0.841\n",
      "2021-12-26T12:28:25.203517: Epoch   3 Batch  825/3125   train_loss = 0.923\n",
      "2021-12-26T12:28:25.960749: Epoch   3 Batch  845/3125   train_loss = 0.918\n",
      "2021-12-26T12:28:26.747311: Epoch   3 Batch  865/3125   train_loss = 1.029\n",
      "2021-12-26T12:28:27.517268: Epoch   3 Batch  885/3125   train_loss = 0.935\n",
      "2021-12-26T12:28:28.269451: Epoch   3 Batch  905/3125   train_loss = 0.997\n",
      "2021-12-26T12:28:29.029313: Epoch   3 Batch  925/3125   train_loss = 0.872\n",
      "2021-12-26T12:28:29.807263: Epoch   3 Batch  945/3125   train_loss = 0.949\n",
      "2021-12-26T12:28:30.577511: Epoch   3 Batch  965/3125   train_loss = 0.798\n",
      "2021-12-26T12:28:31.332266: Epoch   3 Batch  985/3125   train_loss = 0.973\n",
      "2021-12-26T12:28:32.099196: Epoch   3 Batch 1005/3125   train_loss = 0.839\n",
      "2021-12-26T12:28:32.894074: Epoch   3 Batch 1025/3125   train_loss = 0.893\n",
      "2021-12-26T12:28:33.651089: Epoch   3 Batch 1045/3125   train_loss = 1.094\n",
      "2021-12-26T12:28:34.385109: Epoch   3 Batch 1065/3125   train_loss = 0.926\n",
      "2021-12-26T12:28:35.178920: Epoch   3 Batch 1085/3125   train_loss = 0.814\n",
      "2021-12-26T12:28:35.970037: Epoch   3 Batch 1105/3125   train_loss = 0.848\n",
      "2021-12-26T12:28:36.690146: Epoch   3 Batch 1125/3125   train_loss = 0.871\n",
      "2021-12-26T12:28:37.434643: Epoch   3 Batch 1145/3125   train_loss = 0.950\n",
      "2021-12-26T12:28:38.198767: Epoch   3 Batch 1165/3125   train_loss = 1.000\n",
      "2021-12-26T12:28:38.947760: Epoch   3 Batch 1185/3125   train_loss = 0.855\n",
      "2021-12-26T12:28:39.716585: Epoch   3 Batch 1205/3125   train_loss = 0.864\n",
      "2021-12-26T12:28:40.491361: Epoch   3 Batch 1225/3125   train_loss = 0.960\n",
      "2021-12-26T12:28:41.265011: Epoch   3 Batch 1245/3125   train_loss = 1.020\n",
      "2021-12-26T12:28:42.038390: Epoch   3 Batch 1265/3125   train_loss = 0.918\n",
      "2021-12-26T12:28:42.800188: Epoch   3 Batch 1285/3125   train_loss = 1.029\n",
      "2021-12-26T12:28:43.568658: Epoch   3 Batch 1305/3125   train_loss = 0.809\n",
      "2021-12-26T12:28:44.357806: Epoch   3 Batch 1325/3125   train_loss = 0.891\n",
      "2021-12-26T12:28:45.133036: Epoch   3 Batch 1345/3125   train_loss = 0.963\n",
      "2021-12-26T12:28:45.900399: Epoch   3 Batch 1365/3125   train_loss = 0.824\n",
      "2021-12-26T12:28:46.664810: Epoch   3 Batch 1385/3125   train_loss = 0.798\n",
      "2021-12-26T12:28:47.452908: Epoch   3 Batch 1405/3125   train_loss = 0.880\n",
      "2021-12-26T12:28:48.228513: Epoch   3 Batch 1425/3125   train_loss = 0.986\n",
      "2021-12-26T12:28:48.998182: Epoch   3 Batch 1445/3125   train_loss = 1.011\n",
      "2021-12-26T12:28:49.824356: Epoch   3 Batch 1465/3125   train_loss = 0.861\n",
      "2021-12-26T12:28:50.580936: Epoch   3 Batch 1485/3125   train_loss = 0.969\n",
      "2021-12-26T12:28:51.353911: Epoch   3 Batch 1505/3125   train_loss = 0.787\n",
      "2021-12-26T12:28:52.127198: Epoch   3 Batch 1525/3125   train_loss = 0.825\n",
      "2021-12-26T12:28:52.901611: Epoch   3 Batch 1545/3125   train_loss = 0.882\n",
      "2021-12-26T12:28:53.669350: Epoch   3 Batch 1565/3125   train_loss = 0.939\n",
      "2021-12-26T12:28:54.438998: Epoch   3 Batch 1585/3125   train_loss = 0.844\n",
      "2021-12-26T12:28:55.214503: Epoch   3 Batch 1605/3125   train_loss = 0.919\n",
      "2021-12-26T12:28:56.023118: Epoch   3 Batch 1625/3125   train_loss = 0.929\n",
      "2021-12-26T12:28:56.793077: Epoch   3 Batch 1645/3125   train_loss = 0.970\n",
      "2021-12-26T12:28:57.532929: Epoch   3 Batch 1665/3125   train_loss = 0.922\n",
      "2021-12-26T12:28:58.343585: Epoch   3 Batch 1685/3125   train_loss = 0.991\n",
      "2021-12-26T12:28:59.112611: Epoch   3 Batch 1705/3125   train_loss = 0.923\n",
      "2021-12-26T12:28:59.909181: Epoch   3 Batch 1725/3125   train_loss = 0.875\n",
      "2021-12-26T12:29:00.667044: Epoch   3 Batch 1745/3125   train_loss = 0.827\n",
      "2021-12-26T12:29:01.458047: Epoch   3 Batch 1765/3125   train_loss = 0.824\n",
      "2021-12-26T12:29:02.215052: Epoch   3 Batch 1785/3125   train_loss = 0.989\n",
      "2021-12-26T12:29:02.942134: Epoch   3 Batch 1805/3125   train_loss = 0.934\n",
      "2021-12-26T12:29:03.720450: Epoch   3 Batch 1825/3125   train_loss = 1.058\n",
      "2021-12-26T12:29:04.501095: Epoch   3 Batch 1845/3125   train_loss = 0.930\n",
      "2021-12-26T12:29:05.244422: Epoch   3 Batch 1865/3125   train_loss = 0.766\n",
      "2021-12-26T12:29:06.026617: Epoch   3 Batch 1885/3125   train_loss = 0.983\n",
      "2021-12-26T12:29:06.797118: Epoch   3 Batch 1905/3125   train_loss = 0.816\n",
      "2021-12-26T12:29:07.556765: Epoch   3 Batch 1925/3125   train_loss = 0.853\n",
      "2021-12-26T12:29:08.341949: Epoch   3 Batch 1945/3125   train_loss = 0.971\n",
      "2021-12-26T12:29:09.093081: Epoch   3 Batch 1965/3125   train_loss = 0.797\n",
      "2021-12-26T12:29:09.835175: Epoch   3 Batch 1985/3125   train_loss = 0.895\n",
      "2021-12-26T12:29:10.602170: Epoch   3 Batch 2005/3125   train_loss = 0.956\n",
      "2021-12-26T12:29:11.360050: Epoch   3 Batch 2025/3125   train_loss = 0.904\n",
      "2021-12-26T12:29:12.128707: Epoch   3 Batch 2045/3125   train_loss = 0.762\n",
      "2021-12-26T12:29:12.909085: Epoch   3 Batch 2065/3125   train_loss = 0.780\n",
      "2021-12-26T12:29:13.678975: Epoch   3 Batch 2085/3125   train_loss = 1.057\n",
      "2021-12-26T12:29:14.453639: Epoch   3 Batch 2105/3125   train_loss = 0.915\n",
      "2021-12-26T12:29:15.205344: Epoch   3 Batch 2125/3125   train_loss = 0.966\n",
      "2021-12-26T12:29:15.977789: Epoch   3 Batch 2145/3125   train_loss = 0.959\n",
      "2021-12-26T12:29:16.736631: Epoch   3 Batch 2165/3125   train_loss = 0.843\n",
      "2021-12-26T12:29:17.520927: Epoch   3 Batch 2185/3125   train_loss = 0.921\n",
      "2021-12-26T12:29:18.255452: Epoch   3 Batch 2205/3125   train_loss = 0.939\n",
      "2021-12-26T12:29:18.998596: Epoch   3 Batch 2225/3125   train_loss = 0.938\n",
      "2021-12-26T12:29:19.776612: Epoch   3 Batch 2245/3125   train_loss = 0.835\n",
      "2021-12-26T12:29:20.544401: Epoch   3 Batch 2265/3125   train_loss = 0.930\n",
      "2021-12-26T12:29:21.312051: Epoch   3 Batch 2285/3125   train_loss = 1.049\n",
      "2021-12-26T12:29:22.069083: Epoch   3 Batch 2305/3125   train_loss = 0.815\n",
      "2021-12-26T12:29:22.815263: Epoch   3 Batch 2325/3125   train_loss = 0.795\n",
      "2021-12-26T12:29:23.584728: Epoch   3 Batch 2345/3125   train_loss = 0.865\n",
      "2021-12-26T12:29:24.351928: Epoch   3 Batch 2365/3125   train_loss = 0.742\n",
      "2021-12-26T12:29:25.125885: Epoch   3 Batch 2385/3125   train_loss = 0.896\n",
      "2021-12-26T12:29:25.888840: Epoch   3 Batch 2405/3125   train_loss = 0.890\n",
      "2021-12-26T12:29:26.673785: Epoch   3 Batch 2425/3125   train_loss = 0.868\n",
      "2021-12-26T12:29:27.436854: Epoch   3 Batch 2445/3125   train_loss = 0.971\n",
      "2021-12-26T12:29:28.186740: Epoch   3 Batch 2465/3125   train_loss = 0.761\n",
      "2021-12-26T12:29:28.949298: Epoch   3 Batch 2485/3125   train_loss = 0.792\n",
      "2021-12-26T12:29:29.729405: Epoch   3 Batch 2505/3125   train_loss = 0.835\n",
      "2021-12-26T12:29:30.508690: Epoch   3 Batch 2525/3125   train_loss = 0.862\n",
      "2021-12-26T12:29:31.292134: Epoch   3 Batch 2545/3125   train_loss = 1.024\n",
      "2021-12-26T12:29:32.060297: Epoch   3 Batch 2565/3125   train_loss = 0.834\n",
      "2021-12-26T12:29:32.797960: Epoch   3 Batch 2585/3125   train_loss = 0.826\n",
      "2021-12-26T12:29:33.573314: Epoch   3 Batch 2605/3125   train_loss = 0.844\n",
      "2021-12-26T12:29:34.333183: Epoch   3 Batch 2625/3125   train_loss = 1.004\n",
      "2021-12-26T12:29:35.088695: Epoch   3 Batch 2645/3125   train_loss = 0.905\n",
      "2021-12-26T12:29:35.835336: Epoch   3 Batch 2665/3125   train_loss = 0.989\n",
      "2021-12-26T12:29:36.608166: Epoch   3 Batch 2685/3125   train_loss = 0.927\n",
      "2021-12-26T12:29:37.396755: Epoch   3 Batch 2705/3125   train_loss = 0.782\n",
      "2021-12-26T12:29:38.189343: Epoch   3 Batch 2725/3125   train_loss = 0.962\n",
      "2021-12-26T12:29:38.953091: Epoch   3 Batch 2745/3125   train_loss = 0.911\n",
      "2021-12-26T12:29:39.721124: Epoch   3 Batch 2765/3125   train_loss = 0.800\n",
      "2021-12-26T12:29:40.495978: Epoch   3 Batch 2785/3125   train_loss = 0.982\n",
      "2021-12-26T12:29:41.260991: Epoch   3 Batch 2805/3125   train_loss = 0.840\n",
      "2021-12-26T12:29:42.003122: Epoch   3 Batch 2825/3125   train_loss = 0.847\n",
      "2021-12-26T12:29:42.740669: Epoch   3 Batch 2845/3125   train_loss = 0.884\n",
      "2021-12-26T12:29:43.477374: Epoch   3 Batch 2865/3125   train_loss = 0.869\n",
      "2021-12-26T12:29:44.225429: Epoch   3 Batch 2885/3125   train_loss = 0.889\n",
      "2021-12-26T12:29:44.982842: Epoch   3 Batch 2905/3125   train_loss = 0.925\n",
      "2021-12-26T12:29:45.746271: Epoch   3 Batch 2925/3125   train_loss = 0.940\n",
      "2021-12-26T12:29:46.512344: Epoch   3 Batch 2945/3125   train_loss = 0.953\n",
      "2021-12-26T12:29:47.338684: Epoch   3 Batch 2965/3125   train_loss = 0.983\n",
      "2021-12-26T12:29:48.099839: Epoch   3 Batch 2985/3125   train_loss = 0.845\n",
      "2021-12-26T12:29:48.873172: Epoch   3 Batch 3005/3125   train_loss = 0.801\n",
      "2021-12-26T12:29:49.616972: Epoch   3 Batch 3025/3125   train_loss = 0.915\n",
      "2021-12-26T12:29:50.385676: Epoch   3 Batch 3045/3125   train_loss = 0.935\n",
      "2021-12-26T12:29:51.159527: Epoch   3 Batch 3065/3125   train_loss = 0.827\n",
      "2021-12-26T12:29:51.913645: Epoch   3 Batch 3085/3125   train_loss = 0.881\n",
      "2021-12-26T12:29:52.664637: Epoch   3 Batch 3105/3125   train_loss = 0.928\n",
      "2021-12-26T12:29:53.607390: Epoch   3 Batch   17/781   test_loss = 0.906\n",
      "2021-12-26T12:29:53.846921: Epoch   3 Batch   37/781   test_loss = 0.902\n",
      "2021-12-26T12:29:54.100503: Epoch   3 Batch   57/781   test_loss = 0.916\n",
      "2021-12-26T12:29:54.344382: Epoch   3 Batch   77/781   test_loss = 0.861\n",
      "2021-12-26T12:29:54.609366: Epoch   3 Batch   97/781   test_loss = 0.797\n",
      "2021-12-26T12:29:54.858844: Epoch   3 Batch  117/781   test_loss = 0.954\n",
      "2021-12-26T12:29:55.085516: Epoch   3 Batch  137/781   test_loss = 0.934\n",
      "2021-12-26T12:29:55.311389: Epoch   3 Batch  157/781   test_loss = 0.887\n",
      "2021-12-26T12:29:55.544009: Epoch   3 Batch  177/781   test_loss = 0.862\n",
      "2021-12-26T12:29:55.773073: Epoch   3 Batch  197/781   test_loss = 0.920\n",
      "2021-12-26T12:29:56.002574: Epoch   3 Batch  217/781   test_loss = 0.740\n",
      "2021-12-26T12:29:56.269880: Epoch   3 Batch  237/781   test_loss = 0.792\n",
      "2021-12-26T12:29:56.524180: Epoch   3 Batch  257/781   test_loss = 1.004\n",
      "2021-12-26T12:29:56.753064: Epoch   3 Batch  277/781   test_loss = 1.002\n",
      "2021-12-26T12:29:57.002852: Epoch   3 Batch  297/781   test_loss = 1.051\n",
      "2021-12-26T12:29:57.234969: Epoch   3 Batch  317/781   test_loss = 1.067\n",
      "2021-12-26T12:29:57.500918: Epoch   3 Batch  337/781   test_loss = 0.926\n",
      "2021-12-26T12:29:57.766311: Epoch   3 Batch  357/781   test_loss = 0.911\n",
      "2021-12-26T12:29:57.998049: Epoch   3 Batch  377/781   test_loss = 0.954\n",
      "2021-12-26T12:29:58.195094: Epoch   3 Batch  397/781   test_loss = 0.943\n",
      "2021-12-26T12:29:58.425683: Epoch   3 Batch  417/781   test_loss = 0.858\n",
      "2021-12-26T12:29:58.668038: Epoch   3 Batch  437/781   test_loss = 0.792\n",
      "2021-12-26T12:29:58.897486: Epoch   3 Batch  457/781   test_loss = 0.772\n",
      "2021-12-26T12:29:59.144330: Epoch   3 Batch  477/781   test_loss = 0.880\n",
      "2021-12-26T12:29:59.382288: Epoch   3 Batch  497/781   test_loss = 0.805\n",
      "2021-12-26T12:29:59.608607: Epoch   3 Batch  517/781   test_loss = 0.824\n",
      "2021-12-26T12:29:59.841332: Epoch   3 Batch  537/781   test_loss = 0.840\n",
      "2021-12-26T12:30:00.087818: Epoch   3 Batch  557/781   test_loss = 1.016\n",
      "2021-12-26T12:30:00.317501: Epoch   3 Batch  577/781   test_loss = 0.898\n",
      "2021-12-26T12:30:00.559889: Epoch   3 Batch  597/781   test_loss = 0.860\n",
      "2021-12-26T12:30:00.807868: Epoch   3 Batch  617/781   test_loss = 0.911\n",
      "2021-12-26T12:30:01.066646: Epoch   3 Batch  637/781   test_loss = 0.806\n",
      "2021-12-26T12:30:01.307130: Epoch   3 Batch  657/781   test_loss = 0.976\n",
      "2021-12-26T12:30:01.544781: Epoch   3 Batch  677/781   test_loss = 0.881\n",
      "2021-12-26T12:30:01.779596: Epoch   3 Batch  697/781   test_loss = 0.923\n",
      "2021-12-26T12:30:02.031769: Epoch   3 Batch  717/781   test_loss = 0.864\n",
      "2021-12-26T12:30:02.285619: Epoch   3 Batch  737/781   test_loss = 0.780\n",
      "2021-12-26T12:30:02.527268: Epoch   3 Batch  757/781   test_loss = 1.094\n",
      "2021-12-26T12:30:02.780076: Epoch   3 Batch  777/781   test_loss = 0.908\n",
      "2021-12-26T12:30:03.798525: Epoch   4 Batch    0/3125   train_loss = 0.924\n",
      "2021-12-26T12:30:04.606974: Epoch   4 Batch   20/3125   train_loss = 0.899\n",
      "2021-12-26T12:30:05.376948: Epoch   4 Batch   40/3125   train_loss = 0.889\n",
      "2021-12-26T12:30:06.131100: Epoch   4 Batch   60/3125   train_loss = 0.750\n",
      "2021-12-26T12:30:06.912001: Epoch   4 Batch   80/3125   train_loss = 0.863\n",
      "2021-12-26T12:30:07.672020: Epoch   4 Batch  100/3125   train_loss = 0.983\n",
      "2021-12-26T12:30:08.457950: Epoch   4 Batch  120/3125   train_loss = 0.960\n",
      "2021-12-26T12:30:09.231811: Epoch   4 Batch  140/3125   train_loss = 0.946\n",
      "2021-12-26T12:30:10.004060: Epoch   4 Batch  160/3125   train_loss = 0.752\n",
      "2021-12-26T12:30:10.755709: Epoch   4 Batch  180/3125   train_loss = 0.899\n",
      "2021-12-26T12:30:11.511027: Epoch   4 Batch  200/3125   train_loss = 1.109\n",
      "2021-12-26T12:30:12.271380: Epoch   4 Batch  220/3125   train_loss = 0.908\n",
      "2021-12-26T12:30:13.045454: Epoch   4 Batch  240/3125   train_loss = 0.985\n",
      "2021-12-26T12:30:13.790703: Epoch   4 Batch  260/3125   train_loss = 0.950\n",
      "2021-12-26T12:30:14.538043: Epoch   4 Batch  280/3125   train_loss = 1.002\n",
      "2021-12-26T12:30:15.292441: Epoch   4 Batch  300/3125   train_loss = 1.057\n",
      "2021-12-26T12:30:16.074187: Epoch   4 Batch  320/3125   train_loss = 0.963\n",
      "2021-12-26T12:30:16.840256: Epoch   4 Batch  340/3125   train_loss = 0.725\n",
      "2021-12-26T12:30:17.642912: Epoch   4 Batch  360/3125   train_loss = 0.860\n",
      "2021-12-26T12:30:18.408824: Epoch   4 Batch  380/3125   train_loss = 0.861\n",
      "2021-12-26T12:30:19.178516: Epoch   4 Batch  400/3125   train_loss = 0.856\n",
      "2021-12-26T12:30:19.952524: Epoch   4 Batch  420/3125   train_loss = 0.812\n",
      "2021-12-26T12:30:20.684679: Epoch   4 Batch  440/3125   train_loss = 0.850\n",
      "2021-12-26T12:30:21.447521: Epoch   4 Batch  460/3125   train_loss = 0.872\n",
      "2021-12-26T12:30:22.206703: Epoch   4 Batch  480/3125   train_loss = 0.910\n",
      "2021-12-26T12:30:22.991976: Epoch   4 Batch  500/3125   train_loss = 0.671\n",
      "2021-12-26T12:30:23.742221: Epoch   4 Batch  520/3125   train_loss = 0.921\n",
      "2021-12-26T12:30:24.490281: Epoch   4 Batch  540/3125   train_loss = 0.815\n",
      "2021-12-26T12:30:25.229375: Epoch   4 Batch  560/3125   train_loss = 1.018\n",
      "2021-12-26T12:30:25.978534: Epoch   4 Batch  580/3125   train_loss = 0.933\n",
      "2021-12-26T12:30:26.729221: Epoch   4 Batch  600/3125   train_loss = 0.918\n",
      "2021-12-26T12:30:27.503412: Epoch   4 Batch  620/3125   train_loss = 0.925\n",
      "2021-12-26T12:30:28.278481: Epoch   4 Batch  640/3125   train_loss = 0.848\n",
      "2021-12-26T12:30:29.051954: Epoch   4 Batch  660/3125   train_loss = 0.909\n",
      "2021-12-26T12:30:29.847824: Epoch   4 Batch  680/3125   train_loss = 0.896\n",
      "2021-12-26T12:30:30.608081: Epoch   4 Batch  700/3125   train_loss = 0.902\n",
      "2021-12-26T12:30:31.357697: Epoch   4 Batch  720/3125   train_loss = 0.761\n",
      "2021-12-26T12:30:32.108884: Epoch   4 Batch  740/3125   train_loss = 0.925\n",
      "2021-12-26T12:30:32.867548: Epoch   4 Batch  760/3125   train_loss = 0.780\n",
      "2021-12-26T12:30:33.631434: Epoch   4 Batch  780/3125   train_loss = 0.914\n",
      "2021-12-26T12:30:34.396303: Epoch   4 Batch  800/3125   train_loss = 0.803\n",
      "2021-12-26T12:30:35.141803: Epoch   4 Batch  820/3125   train_loss = 0.865\n",
      "2021-12-26T12:30:35.927362: Epoch   4 Batch  840/3125   train_loss = 0.788\n",
      "2021-12-26T12:30:36.675438: Epoch   4 Batch  860/3125   train_loss = 0.833\n",
      "2021-12-26T12:30:37.441819: Epoch   4 Batch  880/3125   train_loss = 0.803\n",
      "2021-12-26T12:30:38.196086: Epoch   4 Batch  900/3125   train_loss = 0.921\n",
      "2021-12-26T12:30:38.955699: Epoch   4 Batch  920/3125   train_loss = 0.952\n",
      "2021-12-26T12:30:39.758856: Epoch   4 Batch  940/3125   train_loss = 0.887\n",
      "2021-12-26T12:30:40.519113: Epoch   4 Batch  960/3125   train_loss = 0.914\n",
      "2021-12-26T12:30:41.287408: Epoch   4 Batch  980/3125   train_loss = 1.025\n",
      "2021-12-26T12:30:42.073760: Epoch   4 Batch 1000/3125   train_loss = 0.989\n",
      "2021-12-26T12:30:42.859453: Epoch   4 Batch 1020/3125   train_loss = 0.926\n",
      "2021-12-26T12:30:43.650339: Epoch   4 Batch 1040/3125   train_loss = 0.823\n",
      "2021-12-26T12:30:44.405386: Epoch   4 Batch 1060/3125   train_loss = 0.944\n",
      "2021-12-26T12:30:45.178776: Epoch   4 Batch 1080/3125   train_loss = 0.906\n",
      "2021-12-26T12:30:45.957173: Epoch   4 Batch 1100/3125   train_loss = 0.853\n",
      "2021-12-26T12:30:46.717184: Epoch   4 Batch 1120/3125   train_loss = 0.865\n",
      "2021-12-26T12:30:47.475323: Epoch   4 Batch 1140/3125   train_loss = 0.879\n",
      "2021-12-26T12:30:48.273671: Epoch   4 Batch 1160/3125   train_loss = 0.832\n",
      "2021-12-26T12:30:49.021197: Epoch   4 Batch 1180/3125   train_loss = 0.888\n",
      "2021-12-26T12:30:49.811993: Epoch   4 Batch 1200/3125   train_loss = 0.976\n",
      "2021-12-26T12:30:50.553200: Epoch   4 Batch 1220/3125   train_loss = 0.951\n",
      "2021-12-26T12:30:51.332663: Epoch   4 Batch 1240/3125   train_loss = 0.787\n",
      "2021-12-26T12:30:52.102402: Epoch   4 Batch 1260/3125   train_loss = 0.882\n",
      "2021-12-26T12:30:52.851494: Epoch   4 Batch 1280/3125   train_loss = 0.902\n",
      "2021-12-26T12:30:53.609218: Epoch   4 Batch 1300/3125   train_loss = 0.831\n",
      "2021-12-26T12:30:54.373965: Epoch   4 Batch 1320/3125   train_loss = 0.880\n",
      "2021-12-26T12:30:55.140215: Epoch   4 Batch 1340/3125   train_loss = 0.757\n",
      "2021-12-26T12:30:55.884075: Epoch   4 Batch 1360/3125   train_loss = 0.823\n",
      "2021-12-26T12:30:56.636942: Epoch   4 Batch 1380/3125   train_loss = 0.808\n",
      "2021-12-26T12:30:57.423225: Epoch   4 Batch 1400/3125   train_loss = 0.905\n",
      "2021-12-26T12:30:58.201247: Epoch   4 Batch 1420/3125   train_loss = 0.908\n",
      "2021-12-26T12:30:58.947953: Epoch   4 Batch 1440/3125   train_loss = 0.787\n",
      "2021-12-26T12:30:59.702324: Epoch   4 Batch 1460/3125   train_loss = 0.885\n",
      "2021-12-26T12:31:00.458420: Epoch   4 Batch 1480/3125   train_loss = 0.924\n",
      "2021-12-26T12:31:01.243857: Epoch   4 Batch 1500/3125   train_loss = 0.936\n",
      "2021-12-26T12:31:02.000747: Epoch   4 Batch 1520/3125   train_loss = 0.792\n",
      "2021-12-26T12:31:02.714963: Epoch   4 Batch 1540/3125   train_loss = 0.981\n",
      "2021-12-26T12:31:03.438428: Epoch   4 Batch 1560/3125   train_loss = 0.813\n",
      "2021-12-26T12:31:04.179158: Epoch   4 Batch 1580/3125   train_loss = 0.991\n",
      "2021-12-26T12:31:04.944188: Epoch   4 Batch 1600/3125   train_loss = 0.796\n",
      "2021-12-26T12:31:05.708767: Epoch   4 Batch 1620/3125   train_loss = 0.754\n",
      "2021-12-26T12:31:06.479394: Epoch   4 Batch 1640/3125   train_loss = 0.991\n",
      "2021-12-26T12:31:07.290521: Epoch   4 Batch 1660/3125   train_loss = 0.945\n",
      "2021-12-26T12:31:08.080900: Epoch   4 Batch 1680/3125   train_loss = 0.875\n",
      "2021-12-26T12:31:08.847087: Epoch   4 Batch 1700/3125   train_loss = 0.808\n",
      "2021-12-26T12:31:09.635007: Epoch   4 Batch 1720/3125   train_loss = 0.898\n",
      "2021-12-26T12:31:10.434501: Epoch   4 Batch 1740/3125   train_loss = 0.965\n",
      "2021-12-26T12:31:11.229422: Epoch   4 Batch 1760/3125   train_loss = 0.826\n",
      "2021-12-26T12:31:12.013103: Epoch   4 Batch 1780/3125   train_loss = 0.914\n",
      "2021-12-26T12:31:12.805484: Epoch   4 Batch 1800/3125   train_loss = 0.832\n",
      "2021-12-26T12:31:13.596743: Epoch   4 Batch 1820/3125   train_loss = 0.808\n",
      "2021-12-26T12:31:14.393823: Epoch   4 Batch 1840/3125   train_loss = 0.920\n",
      "2021-12-26T12:31:15.185889: Epoch   4 Batch 1860/3125   train_loss = 0.980\n",
      "2021-12-26T12:31:15.975345: Epoch   4 Batch 1880/3125   train_loss = 0.901\n",
      "2021-12-26T12:31:16.747313: Epoch   4 Batch 1900/3125   train_loss = 0.764\n",
      "2021-12-26T12:31:17.525179: Epoch   4 Batch 1920/3125   train_loss = 0.857\n",
      "2021-12-26T12:31:18.333935: Epoch   4 Batch 1940/3125   train_loss = 0.791\n",
      "2021-12-26T12:31:19.105391: Epoch   4 Batch 1960/3125   train_loss = 0.780\n",
      "2021-12-26T12:31:19.894895: Epoch   4 Batch 1980/3125   train_loss = 0.870\n",
      "2021-12-26T12:31:20.682113: Epoch   4 Batch 2000/3125   train_loss = 1.022\n",
      "2021-12-26T12:31:21.472847: Epoch   4 Batch 2020/3125   train_loss = 0.959\n",
      "2021-12-26T12:31:22.252608: Epoch   4 Batch 2040/3125   train_loss = 0.823\n",
      "2021-12-26T12:31:22.997303: Epoch   4 Batch 2060/3125   train_loss = 0.805\n",
      "2021-12-26T12:31:23.786771: Epoch   4 Batch 2080/3125   train_loss = 0.990\n",
      "2021-12-26T12:31:24.553615: Epoch   4 Batch 2100/3125   train_loss = 0.830\n",
      "2021-12-26T12:31:25.297609: Epoch   4 Batch 2120/3125   train_loss = 0.783\n",
      "2021-12-26T12:31:26.090710: Epoch   4 Batch 2140/3125   train_loss = 0.845\n",
      "2021-12-26T12:31:26.868133: Epoch   4 Batch 2160/3125   train_loss = 0.835\n",
      "2021-12-26T12:31:27.626960: Epoch   4 Batch 2180/3125   train_loss = 0.918\n",
      "2021-12-26T12:31:28.401112: Epoch   4 Batch 2200/3125   train_loss = 0.819\n",
      "2021-12-26T12:31:29.190538: Epoch   4 Batch 2220/3125   train_loss = 0.829\n",
      "2021-12-26T12:31:29.946865: Epoch   4 Batch 2240/3125   train_loss = 0.827\n",
      "2021-12-26T12:31:30.720049: Epoch   4 Batch 2260/3125   train_loss = 0.915\n",
      "2021-12-26T12:31:31.484599: Epoch   4 Batch 2280/3125   train_loss = 0.904\n",
      "2021-12-26T12:31:32.265557: Epoch   4 Batch 2300/3125   train_loss = 0.869\n",
      "2021-12-26T12:31:33.046503: Epoch   4 Batch 2320/3125   train_loss = 0.977\n",
      "2021-12-26T12:31:33.838275: Epoch   4 Batch 2340/3125   train_loss = 0.869\n",
      "2021-12-26T12:31:34.662494: Epoch   4 Batch 2360/3125   train_loss = 0.886\n",
      "2021-12-26T12:31:35.432049: Epoch   4 Batch 2380/3125   train_loss = 0.847\n",
      "2021-12-26T12:31:36.219501: Epoch   4 Batch 2400/3125   train_loss = 0.909\n",
      "2021-12-26T12:31:37.003902: Epoch   4 Batch 2420/3125   train_loss = 0.842\n",
      "2021-12-26T12:31:37.801417: Epoch   4 Batch 2440/3125   train_loss = 0.790\n",
      "2021-12-26T12:31:38.564812: Epoch   4 Batch 2460/3125   train_loss = 0.875\n",
      "2021-12-26T12:31:39.331222: Epoch   4 Batch 2480/3125   train_loss = 0.963\n",
      "2021-12-26T12:31:40.092023: Epoch   4 Batch 2500/3125   train_loss = 0.807\n",
      "2021-12-26T12:31:40.845947: Epoch   4 Batch 2520/3125   train_loss = 0.925\n",
      "2021-12-26T12:31:41.600808: Epoch   4 Batch 2540/3125   train_loss = 0.837\n",
      "2021-12-26T12:31:42.366145: Epoch   4 Batch 2560/3125   train_loss = 0.683\n",
      "2021-12-26T12:31:43.134652: Epoch   4 Batch 2580/3125   train_loss = 0.869\n",
      "2021-12-26T12:31:43.903943: Epoch   4 Batch 2600/3125   train_loss = 0.862\n",
      "2021-12-26T12:31:44.645859: Epoch   4 Batch 2620/3125   train_loss = 0.799\n",
      "2021-12-26T12:31:45.444902: Epoch   4 Batch 2640/3125   train_loss = 0.816\n",
      "2021-12-26T12:31:46.236503: Epoch   4 Batch 2660/3125   train_loss = 1.021\n",
      "2021-12-26T12:31:47.039069: Epoch   4 Batch 2680/3125   train_loss = 0.784\n",
      "2021-12-26T12:31:47.784544: Epoch   4 Batch 2700/3125   train_loss = 0.913\n",
      "2021-12-26T12:31:48.572924: Epoch   4 Batch 2720/3125   train_loss = 0.764\n",
      "2021-12-26T12:31:49.383013: Epoch   4 Batch 2740/3125   train_loss = 0.874\n",
      "2021-12-26T12:31:50.175101: Epoch   4 Batch 2760/3125   train_loss = 0.811\n",
      "2021-12-26T12:31:51.027060: Epoch   4 Batch 2780/3125   train_loss = 0.872\n",
      "2021-12-26T12:31:51.784968: Epoch   4 Batch 2800/3125   train_loss = 1.014\n",
      "2021-12-26T12:31:52.563884: Epoch   4 Batch 2820/3125   train_loss = 1.037\n",
      "2021-12-26T12:31:53.327128: Epoch   4 Batch 2840/3125   train_loss = 0.843\n",
      "2021-12-26T12:31:54.109369: Epoch   4 Batch 2860/3125   train_loss = 0.797\n",
      "2021-12-26T12:31:54.873189: Epoch   4 Batch 2880/3125   train_loss = 0.903\n",
      "2021-12-26T12:31:55.634887: Epoch   4 Batch 2900/3125   train_loss = 0.887\n",
      "2021-12-26T12:31:56.407190: Epoch   4 Batch 2920/3125   train_loss = 0.870\n",
      "2021-12-26T12:31:57.225216: Epoch   4 Batch 2940/3125   train_loss = 0.933\n",
      "2021-12-26T12:31:58.010706: Epoch   4 Batch 2960/3125   train_loss = 0.885\n",
      "2021-12-26T12:31:58.799844: Epoch   4 Batch 2980/3125   train_loss = 0.844\n",
      "2021-12-26T12:31:59.593162: Epoch   4 Batch 3000/3125   train_loss = 0.922\n",
      "2021-12-26T12:32:00.374395: Epoch   4 Batch 3020/3125   train_loss = 0.995\n",
      "2021-12-26T12:32:01.160883: Epoch   4 Batch 3040/3125   train_loss = 0.940\n",
      "2021-12-26T12:32:01.949819: Epoch   4 Batch 3060/3125   train_loss = 0.725\n",
      "2021-12-26T12:32:02.753197: Epoch   4 Batch 3080/3125   train_loss = 0.977\n",
      "2021-12-26T12:32:03.537827: Epoch   4 Batch 3100/3125   train_loss = 1.000\n",
      "2021-12-26T12:32:04.348251: Epoch   4 Batch 3120/3125   train_loss = 0.813\n",
      "2021-12-26T12:32:04.713177: Epoch   4 Batch   16/781   test_loss = 0.810\n",
      "2021-12-26T12:32:04.965490: Epoch   4 Batch   36/781   test_loss = 0.906\n",
      "2021-12-26T12:32:05.224669: Epoch   4 Batch   56/781   test_loss = 0.931\n",
      "2021-12-26T12:32:05.475718: Epoch   4 Batch   76/781   test_loss = 0.975\n",
      "2021-12-26T12:32:05.727231: Epoch   4 Batch   96/781   test_loss = 1.016\n",
      "2021-12-26T12:32:05.985681: Epoch   4 Batch  116/781   test_loss = 0.839\n",
      "2021-12-26T12:32:06.226618: Epoch   4 Batch  136/781   test_loss = 0.824\n",
      "2021-12-26T12:32:06.478605: Epoch   4 Batch  156/781   test_loss = 0.915\n",
      "2021-12-26T12:32:06.730395: Epoch   4 Batch  176/781   test_loss = 0.877\n",
      "2021-12-26T12:32:06.969981: Epoch   4 Batch  196/781   test_loss = 0.797\n",
      "2021-12-26T12:32:07.197374: Epoch   4 Batch  216/781   test_loss = 1.015\n",
      "2021-12-26T12:32:07.442049: Epoch   4 Batch  236/781   test_loss = 0.793\n",
      "2021-12-26T12:32:07.690803: Epoch   4 Batch  256/781   test_loss = 0.805\n",
      "2021-12-26T12:32:07.941988: Epoch   4 Batch  276/781   test_loss = 1.073\n",
      "2021-12-26T12:32:08.197767: Epoch   4 Batch  296/781   test_loss = 0.818\n",
      "2021-12-26T12:32:08.470669: Epoch   4 Batch  316/781   test_loss = 0.870\n",
      "2021-12-26T12:32:08.719722: Epoch   4 Batch  336/781   test_loss = 0.759\n",
      "2021-12-26T12:32:08.981818: Epoch   4 Batch  356/781   test_loss = 0.847\n",
      "2021-12-26T12:32:09.212179: Epoch   4 Batch  376/781   test_loss = 0.900\n",
      "2021-12-26T12:32:09.454164: Epoch   4 Batch  396/781   test_loss = 0.848\n",
      "2021-12-26T12:32:09.712300: Epoch   4 Batch  416/781   test_loss = 0.978\n",
      "2021-12-26T12:32:09.986028: Epoch   4 Batch  436/781   test_loss = 0.890\n",
      "2021-12-26T12:32:10.223550: Epoch   4 Batch  456/781   test_loss = 0.736\n",
      "2021-12-26T12:32:10.477099: Epoch   4 Batch  476/781   test_loss = 0.947\n",
      "2021-12-26T12:32:10.737725: Epoch   4 Batch  496/781   test_loss = 0.966\n",
      "2021-12-26T12:32:10.987722: Epoch   4 Batch  516/781   test_loss = 0.769\n",
      "2021-12-26T12:32:11.241439: Epoch   4 Batch  536/781   test_loss = 0.950\n",
      "2021-12-26T12:32:11.495523: Epoch   4 Batch  556/781   test_loss = 0.835\n",
      "2021-12-26T12:32:11.757680: Epoch   4 Batch  576/781   test_loss = 0.990\n",
      "2021-12-26T12:32:12.004987: Epoch   4 Batch  596/781   test_loss = 0.965\n",
      "2021-12-26T12:32:12.251341: Epoch   4 Batch  616/781   test_loss = 0.957\n",
      "2021-12-26T12:32:12.507039: Epoch   4 Batch  636/781   test_loss = 0.887\n",
      "2021-12-26T12:32:12.736558: Epoch   4 Batch  656/781   test_loss = 0.924\n",
      "2021-12-26T12:32:12.957251: Epoch   4 Batch  676/781   test_loss = 1.056\n",
      "2021-12-26T12:32:13.207560: Epoch   4 Batch  696/781   test_loss = 0.861\n",
      "2021-12-26T12:32:13.449846: Epoch   4 Batch  716/781   test_loss = 0.910\n",
      "2021-12-26T12:32:13.682934: Epoch   4 Batch  736/781   test_loss = 1.011\n",
      "2021-12-26T12:32:13.921962: Epoch   4 Batch  756/781   test_loss = 0.855\n",
      "2021-12-26T12:32:14.146920: Epoch   4 Batch  776/781   test_loss = 0.715\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "losses = {'train':[], 'test':[]}\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    #为使用可视化工具tensorBoard搜集数据\n",
    "    grad_summaries = []# 用于保存过程中的梯度值\n",
    "    #训练过程中，主要用到了tf.summary()的各类方法，能够保存训练过程以及参数分布图并在tensorboard显示\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    # 确定好模型的输出与summaries的存储路径\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)# 损失函数值\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])# 训练过程中的参数分布图\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):#前面的代码中确定了num_epoch也即训练的轮次=5\n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,targets_values,test_size = 0.2,random_state = 0)\n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step) \n",
    "            # 每show_every_n_batches个batch就输出一次，输出的格式类似于2021-12-26T12:20:59.208584: Epoch0 Batch0/3125train_loss = 21.708\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(time_str,epoch_i,batch_i,(len(train_X) // batch_size),train_loss))\n",
    "        \n",
    "        for batch_i  in range(len(test_X) // batch_size):#开始迭代进行测试\n",
    "            x, y = next(test_batches)\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step) \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(time_str,epoch_i,batch_i,(len(test_X) // batch_size),test_loss))\n",
    "    \n",
    "    saver.save(sess, save_dir)  # 保存模型\n",
    "    print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBV7cSrlqwBY"
   },
   "source": [
    "## 在 TensorBoard 中查看可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz0FNewKqwBY"
   },
   "source": [
    "- 这一步也可省略，因为用tensorboard展示出来的内容和下方的可视化类似，不过需要在本地下载tensoboard并且需要相应的版本\n",
    "- 但是会更好看且可有交互，不过若想保存下来，则会失去交互效果，因此还是采用下方用matplotlib画图\n",
    "- 终端中执行命令 tensorboard --logdir D:/by_CUC/dataMining/mltf/content/runs/1640521250/summaries 即可\n",
    "- 方便起见，还是采用下方用matplotlib画图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y18m0J3lqwBY"
   },
   "source": [
    "## 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Yp5c0_OuqwBY"
   },
   "outputs": [],
   "source": [
    "save_params((save_dir))#保存`save_dir` 在生成预测时使用。\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cg0nZhPqwBZ"
   },
   "source": [
    "## 显示训练Loss损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Xh1yv30oqwBZ",
    "outputId": "40253fe5-f8d8-4bff-ac93-a2a34d683ccf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kxJcy1rqwBZ"
   },
   "source": [
    "## 显示测试Loss损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EmUpS3ycqwBZ",
    "outputId": "89c82d2a-bfa2-4aab-ec4c-3202864b7fb1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gd0VKqlqwBZ"
   },
   "source": [
    "## 获取 Tensors\n",
    "使用函数 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)从 `loaded_graph` 中获取tensors，后面的推荐功能要用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Rsxl2uWEqwBZ"
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\")\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuG8HUJMqwBZ"
   },
   "source": [
    "\n",
    "##  从训练好的模型中得到预测评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jG9xhJvOqwBZ"
   },
   "outputs": [],
   "source": [
    "#可以理解为是网络的正向传播\n",
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')#加载已经保存下来的训练好的模型\n",
    "        loader.restore(sess, load_dir)\n",
    "        # 从加载的模型中获取推荐需要的tensor\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "        # 得到预测的评分rating\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUZ6qssCqwBZ",
    "outputId": "96777431-c65e-4585-d34f-6f0a83e0a676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.2218027]], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUOrnjO4qwBa"
   },
   "source": [
    "\n",
    "### 将训练好的电影特征组合成电影特征矩阵并保存到本地movie_matrics.p文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSRmondHqwBa",
    "outputId": "1f109a97-c733-41ad-c166-8172d584457e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "# 将训练好的电影特征组合成电影特征矩阵并保存到本地movie_matrics.p文件\n",
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess: \n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    # 从加载的模型中获取推荐需要的tensor\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "#将学习得到的电影特征矩阵存入movie_matrics.p文件中\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "frDhbVTVqwBa"
   },
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJVcIjMUqwBa"
   },
   "source": [
    "### 将训练好的用户特征组合成用户特征矩阵并保存到本地users_matrics.p文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsApeDUWqwBa",
    "outputId": "75f8daf1-1225-4993-b15d-aada34411dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    #从加载的模型中获取推荐需要的tensor\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "    for item in users.values:\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EcXflZqgqwBa"
   },
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqmNGZJ7qwBa"
   },
   "source": [
    "## 开始推荐电影\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpdU06CYqwBa"
   },
   "source": [
    "### 推荐同类型的电影\n",
    "先计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个,为了使得每次推荐的内容不会完全一致，因此加入了随机成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "J3M6PuneqwBa"
   },
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # 加载保存下来的模型\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        #先将整个电影特征矩阵模型进行归一化 ，是为了避免整体值太大\n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "        #计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        #取相似度最大的top_k个电影作为推荐的电影\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set() #用来存放最后要进行推荐的电影\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]#为了使得每次推荐的内容不会完全一致，因此加入了随即成分\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY_r7wBxqwBa",
    "outputId": "304c3ec3-9f82-4487-dc03-0664356ecc0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "2689\n",
      "[2758 'Plenty (1985)' 'Drama']\n",
      "296\n",
      "[299 'Priest (1994)' 'Drama']\n",
      "392\n",
      "[396 'Fall Time (1995)' 'Drama']\n",
      "1098\n",
      "[1114 'Funeral, The (1996)' 'Drama']\n",
      "1658\n",
      "[1706 'Harlem River Drive (1996)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{296, 392, 1098, 1658, 2689}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFXu9LFbqwBb"
   },
   "source": [
    "### 推荐某个用户喜欢的电影\n",
    "思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UxljIMnAqwBb"
   },
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  \n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')# 加载已保存好的模型\n",
    "        loader.restore(sess, load_dir)\n",
    "        #推荐某个用户喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))#将两个特征矩阵相乘即可得到评分\n",
    "        sim = (probs_similarity.eval())\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoXMiUAoqwBb",
    "outputId": "50be4d2b-64ec-4f2e-c8c4-1a394659b914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "以下是给您的推荐：\n",
      "1122\n",
      "[1138 'Dadetown (1995)' 'Documentary']\n",
      "2274\n",
      "[2343 'Naked Man, The (1998)' 'Drama']\n",
      "3243\n",
      "[3312 'McCullochs, The (1975)' 'Drama']\n",
      "689\n",
      "[698 'Delta of Venus (1994)' 'Drama']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{689, 1122, 1950, 2274, 3243}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN_FM2yuqwBb"
   },
   "source": [
    "### 喜欢看这个电影的人有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vLrHZ2LeqwBb"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  \n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')# 加载已经保存好的模型\n",
    "        loader.restore(sess, load_dir)\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        #！！！注意这里为了得到针对每一部电影而言，每个用户对其的兴趣程度\n",
    "        #！！！在计算相似度时，应当用电影特征矩阵×用户特征矩阵的转置\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "id": "Abx1AjuHqwBb",
    "outputId": "8e5a0d5b-4bac-4c9e-f275-774fb36b080e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[3833 'M' 25 1]\n",
      " [2002 'F' 56 13]\n",
      " [5458 'F' 18 2]\n",
      " [1855 'M' 18 4]\n",
      " [1644 'M' 18 12]\n",
      " [5102 'M' 25 12]\n",
      " [4967 'M' 18 4]\n",
      " [2312 'M' 45 0]\n",
      " [4281 'M' 56 13]\n",
      " [1763 'M' 35 7]\n",
      " [2258 'F' 18 4]\n",
      " [1383 'F' 25 7]\n",
      " [100 'M' 35 17]\n",
      " [5728 'F' 35 20]\n",
      " [2154 'M' 25 12]\n",
      " [1701 'F' 25 4]\n",
      " [4800 'M' 18 4]\n",
      " [4438 'M' 50 0]\n",
      " [371 'M' 18 4]\n",
      " [4085 'F' 25 6]]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1244, 1742, 1779, 1812, 2535}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BiYS34NHqwBS",
    "6OcO2gnmqwBT",
    "m3jD9pToqwBT",
    "Dl5G0X76qwBU",
    "36aXJv2NqwBU",
    "vJmbg0_3qwBU",
    "VWH0PhR-qwBV",
    "8Pu4F-oHqwBV",
    "8bspeqVIqwBV",
    "QoFFKu1mqwBV",
    "tapfrz5-qwBW",
    "HRzbaePsqwBW",
    "9kXotT3-qwBW",
    "9RlbzJk3qwBW",
    "szp7wgOLqwBW",
    "b40xek-eqwBX",
    "zeGYhp5vqwBX",
    "W8czlwz4qwBY",
    "X_KB3fglqwBY",
    "BBV7cSrlqwBY",
    "Y18m0J3lqwBY",
    "7cg0nZhPqwBZ",
    "_kxJcy1rqwBZ",
    "9gd0VKqlqwBZ",
    "OuG8HUJMqwBZ",
    "SUOrnjO4qwBa",
    "hJVcIjMUqwBa",
    "GqmNGZJ7qwBa",
    "XpdU06CYqwBa",
    "ZFXu9LFbqwBb",
    "jN_FM2yuqwBb",
    "2ds89aSTqwBb"
   ],
   "name": "movie_recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "by",
   "language": "python",
   "name": "by"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
